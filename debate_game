# thought_debate_council_game_v2_1_1.py
from __future__ import annotations # å‹ãƒ’ãƒ³ãƒˆã®é…å»¶è©•ä¾¡ã‚’æœ‰åŠ¹ã«ã™ã‚‹

"""
ğŸ‘‘ è‡ªå¾‹æ€è€ƒè¨è«–ã‚«ãƒ¼ãƒãƒ« - v2.1.1 ğŸ‘‘
   ä½œæˆè€…: nobody@nowhere.net (è‡ªå·±å®Œçµå‹ã®æ¢æ±‚)

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæå‡ºã—ãŸæ€è€ƒã«ã¤ã„ã¦ã€AIé§†å‹•ã®è¨è«–ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€‚
ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯è‡ªå·±å®Œçµå‹ã§è¨­è¨ˆã•ã‚Œã¦ãŠã‚Šã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®æ§‹é€ åŒ–ã•ã‚ŒãŸè­°è«–ã‚’é€šã˜ã¦
ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’æ¢æ±‚ã™ã‚‹ãŸã‚ã®ãƒªãƒƒãƒãªç’°å¢ƒã‚’æä¾›ã—ã¾ã™ã€‚

----------------------------------------------------------------------------------------------------
ğŸš€ ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä½¿ã„æ–¹ (ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³CLIã¾ãŸã¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ã—ã¦) ğŸš€

**I. ã‚³ã‚¢ã‚³ãƒ³ã‚»ãƒ—ãƒˆ:**
   ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç•°ãªã‚‹å½¹å‰²ï¼ˆè³›æˆã€åå¯¾ã€ä¸­ç«‹è©•ä¾¡è€…ã€ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ï¼‰ã‚’æ‹…ã„ã€
   ä¸ãˆã‚‰ã‚ŒãŸã€Œæ€è€ƒã€ã‚’è¤‡æ•°ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã«ã‚ãŸã£ã¦åˆ†æãƒ»è©•ä¾¡ã™ã‚‹ã€Œè¨è«–è©•è­°ä¼šã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¾ã™ã€‚
   ç›®æ¨™ã¯ã€æ€è€ƒã®å¼·åº¦ã‚’åˆ¤æ–­ã—ã€ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ã‚’é”æˆã—ã€ãã®æ„å‘³åˆã„ã‚’æ¢æ±‚ã™ã‚‹ã“ã¨ã§ã™ã€‚

**II. å‰ææ¡ä»¶ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:**

   1. Python 3.9+ ã‚’æ¨å¥¨ (`from __future__ import annotations`ãŒæ¨™æº–å‹•ä½œã®ãŸã‚)ã€‚
   2. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
      pip install openai numpy faiss-cpu tenacity pydantic python-dotenv
      (äº’æ›æ€§ã®ã‚ã‚‹NVIDIA GPUã¨CUDAã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒã‚ã‚‹å ´åˆã¯`faiss-cpu`ã®ä»£ã‚ã‚Šã«`faiss-gpu`ã‚’ä½¿ç”¨)
   3. ã‚°ãƒ©ãƒ•è¦–è¦šåŒ–ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®ãŸã‚ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
      pip install graphviz
      (Graphvizã‚·ã‚¹ãƒ†ãƒ ãƒ„ãƒ¼ãƒ«ã‚‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: https://graphviz.org/download/)

**III. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— - APIã‚­ãƒ¼ (é‡è¦ï¼):**

   ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€LLMã¨æ©Ÿèƒ½ã™ã‚‹ãŸã‚ã«OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚
   - **æ¨å¥¨æ–¹æ³• (ç’°å¢ƒå¤‰æ•°):**
     `OPENAI_API_KEY`ã¨ã„ã†åå‰ã®ç’°å¢ƒå¤‰æ•°ã«æœ‰åŠ¹ãªã‚­ãƒ¼ã‚’è¨­å®šã—ã¾ã™ã€‚
     - Windows (ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ): `set OPENAI_API_KEY=sk-YourActualOpenApiKeyHere`
     - Windows (PowerShell):   `$env:OPENAI_API_KEY="sk-YourActualOpenApiKeyHere"`
     - Linux/macOS (Bash/Zsh): `export OPENAI_API_KEY="sk-YourActualOpenApiKeyHere"`
   - **ä»£æ›¿æ–¹æ³• (.envãƒ•ã‚¡ã‚¤ãƒ«):**
     ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆã¾ãŸã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆï¼‰ã«`.env`ã¨ã„ã†åå‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€
     `OPENAI_API_KEY="sk-YourActualOpenApiKeyHere"`ã¨ã„ã†è¡Œã‚’è¿½åŠ ã—ã¾ã™ã€‚
     (`python-dotenv`ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®æ–¹æ³•ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å…ˆé ­ã«`from dotenv import load_dotenv; load_dotenv()`ã‚’è¿½åŠ ã—ã¦ãã ã•ã„)ã€‚

**IV. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–CLIã‚²ãƒ¼ãƒ ã®å®Ÿè¡Œ:**

   1. ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå«ã¾ã‚Œã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã¾ã™ã€‚
   2. å®Ÿè¡Œ: `python thought_debate_council_game_v2_1_1.py` (ã¾ãŸã¯ã‚ãªãŸã®ãƒ•ã‚¡ã‚¤ãƒ«å)
   3. **ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å…¥åŠ›:** ãƒ¦ãƒ¼ã‚¶ãƒ¼åã®å…¥åŠ›ã‚’æ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚ãªãŸã®è¨è«–ãƒ‡ãƒ¼ã‚¿ã‚’
      åˆ¥ã®ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆä¾‹ï¼š`./council_cli_user_data_v2_1/your_username/`ï¼‰ã«æ•´ç†ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
   4. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¾“ã†:**
      - æ€è€ƒ/ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã‚’å…¥åŠ›ã—ã¦æ–°ã—ã„è¨è«–ã‚’é–‹å§‹ã—ã¾ã™ã€‚
      - ãã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã§ä»¥å‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã¯ã€æ—¢å­˜ã®é€²è¡Œä¸­ã®è¨è«–ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
      - ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’é€²ã‚ãŸã‚Šã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¡¨ç¤ºã—ãŸã‚Šã€è¨è«–ã‚’ãƒ•ã‚©ãƒ¼ã‚¯ã—ãŸã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸã‚Šã€çµè«–ä»˜ã‘ãŸã‚Šã—ã¾ã™ã€‚

   **ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚³ãƒãƒ³ãƒ‰:**
     - `[A]dvance` (ã¾ãŸã¯å˜ã«Enterã‚­ãƒ¼ã‚’æŠ¼ã™): æ¬¡ã®è¨è«–ãƒ©ã‚¦ãƒ³ãƒ‰ã«é€²ã¿ã¾ã™ã€‚
     - `[S]tatus`: è¨è«–ã®ç¾åœ¨ã®è©³ç´°ãªçŠ¶æ…‹ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
     - `[F]ork`: ç¾åœ¨ã®è¨è«–ã«åŸºã¥ã„ã¦ã€å¤‰æ›´ã•ã‚ŒãŸæ€è€ƒã§æ–°ã—ã„è¨è«–ã‚’ä½œæˆã—ã¾ã™ã€‚
     - `[E]xport Map`: è­°è«–ã‚’CSVã¾ãŸã¯JSONLã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
     - `[G]raphviz`: è¨è«–ã®æµã‚Œã‚’Graphviz DOTãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
     - `[C]onclude`: ç¾åœ¨ã®è¨è«–ã‚’çµ‚äº†ã—ã€æœ€çµ‚çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
     - `[Q]uit`: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¾ã™ã€‚

**V. æ§‹é€ åŒ–ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã®å®Ÿè¡Œ:**

   ã“ã®ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€æœŸå¾…ã•ã‚Œã‚‹çµæœã«å¯¾ã—ã¦è¨è«–ã‚·ãƒŠãƒªã‚ªã®è‡ªå‹•ãƒ†ã‚¹ãƒˆãŒå¯èƒ½ã§ã™ã€‚

   1. **ãƒ†ã‚¹ãƒˆè¨ˆç”»JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ:**
      (`run_structured_tests_v2_1`é–¢æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…ã®ä¾‹ã®æ§‹é€ ã‚’å‚ç…§)
      `initial_thought`ã¨`expected_outcome`ã‚’æŒã¤`debate_test_cases`ã‚’å®šç¾©ã—ã¾ã™ã€‚

   2. **å®Ÿè¡Œ:**
      python thought_debate_council_game_v2_1_1.py --test-mode /path/to/your_test_plan.json

   3. **å‡ºåŠ›ã‚’ç¢ºèª:** ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®PASS/FAILã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã—ã€
      ãƒ†ã‚¹ãƒˆã•ã‚ŒãŸå„è¨è«–ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆCSVãƒãƒƒãƒ—ã€DOTã‚°ãƒ©ãƒ•ï¼‰ã‚’ä¿å­˜ã—ã¾ã™ã€‚

**VI. ç‹¬è‡ªã®Pythonãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ã—ã¦ä½¿ç”¨:**

   from thought_debate_council_game_v2_1_1 import ThoughtDebateCouncilGame, OpenAIClient, DEFAULT_GAME_CONFIG_V2_1

   # 1. LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
   my_api_key = os.getenv("MY_PROJECT_OPENAI_KEY") # ã‚­ãƒ¼ã‚’å®‰å…¨ã«ç®¡ç†
   llm_client = OpenAIClient(api_key=my_api_key)

   # 2. è©•è­°ä¼šã‚²ãƒ¼ãƒ ã‚’è¨­å®š
   council_config = DEFAULT_GAME_CONFIG_V2_1.copy()
   council_config["user_data_path_root"] = "./my_app_council_data" # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã¯ã‚¢ãƒ—ãƒªç›¸å¯¾ã§ä¿å­˜
   
   council_game = ThoughtDebateCouncilGame(
       llm_client=llm_client,
       username="my_app_user_session_1", 
       config=council_config
   )

   # 3. å¯¾è©± (ä¾‹: è¨è«–ã‚’é–‹å§‹ã—ã¦1ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œ)
   debate1 = council_game.start_new_debate("ç§‘å­¦çš„ç™ºè¦‹ã«ãŠã‘ã‚‹AIã®å½¹å‰²")
   if debate1['status'] == 'active':
       debate1 = council_game.advance_debate_round(debate1['debate_id'])
   print(json.dumps(debate1, indent=2, default=str))

**VII. ä¸»ãªæ©Ÿèƒ½ã¨è¨­è¨ˆ (v2.1.1):**
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼é–¢é€£ãƒ‡ãƒ¼ã‚¿: è¨è«–/ãƒ­ã‚°ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼åã”ã¨ã«ä¿å­˜ã€‚
    - æ°¸ç¶šçš„ãªçŠ¶æ…‹: é€²è¡Œä¸­ã®è¨è«–ã¯ãƒ­ãƒ¼ãƒ‰/ã‚»ãƒ¼ãƒ–ã•ã‚Œã€çµè«–ãŒå‡ºãŸè¨è«–ã¯ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã•ã‚Œã‚‹ã€‚
    - å‹•çš„ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°: æ€è€ƒã®å¼·åº¦ã¨ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ãŒé€²åŒ–ã™ã‚‹ã€‚
    - è­°è«–FAISS: å€‹ã€…ã®è­°è«–ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨å…±ã«ä¿å­˜ã€‚
    - LLMæŠ½è±¡åŒ–: `BaseLLMClient`ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆOpenAIClientæä¾›ï¼‰ã€‚
    - è¨­å®šå¯èƒ½: ãƒ¢ãƒ‡ãƒ«ã€ãƒšãƒ«ã‚½ãƒŠã€ã—ãã„å€¤ã€ãƒ‘ã‚¹ã‚’è¾æ›¸ã§è¨­å®šã€‚
    - åŸ‹ã‚è¾¼ã¿ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª: æ˜ç¢ºãªLLMæŒ‡ç¤ºã®ãŸã‚ã€‚
    - ãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£ãƒ‰ãƒªãƒ•ãƒˆ: AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ¸©åº¦ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®å¤‰å‹•ã€‚
    - é«˜åº¦ãªã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã¨æ•µå¯¾çš„æ¤œè¨¼ã€‚
    - ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: CSV/JSONLã®è­°è«–ãƒãƒƒãƒ—ã€Graphviz DOTã®è¨è«–ã‚°ãƒ©ãƒ•ã€‚
    - æ§‹é€ åŒ–ãƒ†ã‚¹ãƒˆ: JSONãƒ—ãƒ©ãƒ³ã«ã‚ˆã‚‹è‡ªå‹•ãƒ†ã‚¹ãƒˆã€‚
    - è‡ªå·±å®Œçµå‹: ä¸€èˆ¬çš„ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¾å­˜é–¢ä¿‚ã‚’æŒã¤å˜ä¸€ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ã€‚
----------------------------------------------------------------------------------------------------
"""

import os
import sys
import json
import hashlib
import pickle
import uuid
import time
import logging
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Callable, Union
from abc import ABC, abstractmethod
import random
import csv
import statistics # IMPORTED

# --- ä¾å­˜é–¢ä¿‚ã®ãƒã‚§ãƒƒã‚¯ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError: NUMPY_AVAILABLE = False; np = None
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError: FAISS_AVAILABLE = False; faiss = None
try:
    from openai import OpenAI, APIError as OpenAIAPIError, RateLimitError, APIConnectionError, BadRequestError
    OPENAI_AVAILABLE = True
except ImportError: OPENAI_AVAILABLE = False; OpenAI = OpenAIAPIError = RateLimitError = APIConnectionError = BadRequestError = None # type: ignore
try:
    import tenacity
    TENACITY_AVAILABLE = True
    retry_llm_call = tenacity.retry(
        wait=tenacity.wait_exponential(multiplier=1, min=2, max=30),
        stop=tenacity.stop_after_attempt(3),
        retry=tenacity.retry_if_exception_type((OpenAIAPIError, RateLimitError, APIConnectionError)) if OPENAI_AVAILABLE and OpenAIAPIError else None, # type: ignore
        before_sleep=tenacity.before_sleep_log(logging.getLogger("ThoughtDebateCouncilGameV2.1.1"), logging.WARNING)
    )
except ImportError:
    TENACITY_AVAILABLE = False
    def retry_llm_call(func): return func # type: ignore
    logging.getLogger("ThoughtDebateCouncilGameV2.1.1").warning("Tenacityãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚LLMå‘¼ã³å‡ºã—ã®ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")

try:
    import graphviz # .dotã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®ãŸã‚ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    GRAPHVIZ_AVAILABLE = True
except ImportError:
    GRAPHVIZ_AVAILABLE = False
    logging.getLogger("ThoughtDebateCouncilGameV2.1.1").info("Graphvizãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚DOTãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")


# --- ãƒ­ã‚¬ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
council_logger = logging.getLogger("ThoughtDebateCouncilGameV2.1.1")
if not council_logger.handlers:
    _handler = logging.StreamHandler(sys.stdout)
    _formatter = logging.Formatter('%(asctime)s - %(name)s [%(levelname)s] - [%(filename)s:%(lineno)d] - %(message)s')
    _handler.setFormatter(_formatter)
    council_logger.addHandler(_handler)
    council_logger.setLevel(os.getenv("COUNCIL_GAME_LOG_LEVEL", "INFO").upper())


# --- è¨­å®šã¨å®šæ•° ---
DEFAULT_GAME_CONFIG_V2_1 = {
    "default_llm_model": "gpt-4o-mini",
    "embedding_model_for_faiss": "text-embedding-3-small",
    "embedding_dim": 1536,
    "debate_rounds_max": 3,
    "agents_per_side_per_round": 1,
    "user_data_path_root": "./council_user_data_v2_1",
    "log_to_jsonl": True,
    "jsonl_log_filename": "council_game_events.jsonl",
    "debate_agents": {
        "pro": {"persona": "å…ˆè¦‹ã®æ˜ã‚ã‚‹æ”¯æŒè€…", "temperature": 0.72, "max_tokens": 350},
        "con": {"persona": "ç¾å®Ÿçš„ãªæ‡ç–‘è«–è€…", "temperature": 0.68, "max_tokens": 350},
        "neutral_evaluator": {"persona": "é¦–å¸­è£å®šå®˜", "temperature": 0.3, "max_tokens": 450},
        "argument_recombiner": {"persona": "å‰µé€ çš„çµ±åˆè€…ï¼ˆè¨˜æ†¶ï¼‰", "temperature": 0.6, "max_tokens": 350},
        "adversarial_validator": {"persona": "æ‡ç–‘çš„ãªãƒ¬ãƒƒãƒ‰ãƒãƒ¼ãƒ ", "temperature": 0.7, "max_tokens": 400}
    },
    "advisor_personas": {
        "synthesis": {"persona":"å…¨ä½“çš„ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚¿ãƒ¼", "temperature":0.55, "max_tokens":400},
        "ethics": {"persona":"å€«ç†ã®ç¾…é‡ç›¤", "temperature":0.4, "max_tokens":350},
        "impact": {"persona":"å…ˆè¦‹ã®æ˜ã‚ã‚‹ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆ", "temperature":0.5, "max_tokens":350},
        "fallacy": {"persona":"è«–ç†ã®ç•ªäºº", "temperature":0.3, "max_tokens":300},
        "meta_observer": {"persona":"ãƒ—ãƒ­ã‚»ã‚¹ç›£æŸ»å®˜", "temperature":0.3, "max_tokens":300}
    },
    "initial_thought_score": 50.0,
    "persuasiveness_score_impact_factor": 0.1,
    "consensus_threshold_promote": 70.0,
    "consensus_threshold_reject": 35.0,
    "max_argument_length_for_prompt": 150,
    "faiss_search_top_k_for_recombiner": 3,
    "enable_personality_drift": True,
    "temperature_drift_range": (-0.05, 0.05),
    "max_active_debates_in_memory": 20
}

# --- åŸ‹ã‚è¾¼ã¿ãƒŸãƒ‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
PROMPT_TEMPLATES = {
    "generate_argument": """
ã‚ãªãŸã¯è¨è«–ã®ãƒ©ã‚¦ãƒ³ãƒ‰{{round_num}}ã«ãŠã‘ã‚‹ã€Œ{{persona}}ã€ã¨ã„ã†ãƒšãƒ«ã‚½ãƒŠã‚’æŒã¤AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
è¨è«–ã•ã‚Œã¦ã„ã‚‹å…ƒã®æ€è€ƒã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ï¼šã€Œ{{original_thought}}ã€
ã‚ãªãŸã®å´ï¼ˆ{{agent_type.upper()}}ï¼‰ã®æ—¢å­˜ã®è­°è«–ï¼ˆã‚‚ã—ã‚ã‚Œã°ã€æœ€æ–°ã®ã‚‚ã®ã‹ã‚‰é †ã«ï¼‰ï¼š
{{existing_args_summary}}

ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯ã€æ–°ã—ãã€ç°¡æ½”ã§ã€èª¬å¾—åŠ›ã®ã‚ã‚‹{{agent_type.upper()}}ã®è­°è«–ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚
æ˜ç¢ºãªç‚¹ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã‹ã€ä»¥å‰ã®è­°è«–ã‚’æ‰¹åˆ¤çš„ã«ç™ºå±•ã•ã›ã¦ãã ã•ã„ã€‚å¼·åŠ›ãªè«–ç†ã¨æ–°è¦æ€§ã‚’ç›®æŒ‡ã—ã¦ãã ã•ã„ã€‚
å³å¯†ã«JSONå½¢å¼ã§ã€ã“ã‚Œã‚‰ã®æ­£ç¢ºãªã‚­ãƒ¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{
  "argument_text": "[ã‚ãªãŸã®ç°¡æ½”ãªè­°è«–ã€æœ€å¤§2-3æ–‡ã®æ´å¯Ÿã«å¯Œã‚“ã æ–‡ã€‚ãƒã‚¤ãƒ³ãƒˆã®ç¹°ã‚Šè¿”ã—ã¯é¿ã‘ã¦ãã ã•ã„ã€‚]",
  "persuasiveness_score": "[æµ®å‹•å°æ•°ç‚¹æ•°ï¼šã“ã®ç‰¹å®šã®è­°è«–ãŒæ–°è¦æ€§ã¨å½±éŸ¿ã‚’è€ƒæ…®ã—ã¦ã©ã‚Œã ã‘èª¬å¾—åŠ›ãŒã‚ã‚‹ã‹ã€0.1ã‹ã‚‰1.0ã®è‡ªå·±è©•ä¾¡ã‚¹ã‚³ã‚¢]",
  "keywords": ["[ã‚ãªãŸã®", "è­°è«–ã‹ã‚‰", "3-5å€‹ã®", "ä¸»è¦ãª", "ç”¨èªã®ãƒªã‚¹ãƒˆ"]
}""",
    "evaluate_round": """
ã‚ãªãŸã¯ã€Œ{{persona}}ã€ã§ã™ã€‚è¨è«–ã®è­°é¡Œã¯ï¼šã€Œ{{original_thought}}ã€
ç¾åœ¨ã®æ€è€ƒã®å¼·åº¦ï¼š{{thought_strength:.1f}}/100ã€‚ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ãƒ¬ãƒ™ãƒ«ï¼š{{consensus_level:.2f}}ã€‚
ã“ã‚Œã¯ãƒ©ã‚¦ãƒ³ãƒ‰{{current_round}}ã§ã™ã€‚
ã“ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã®æ–°ã—ã„è³›æˆæ„è¦‹ï¼š
{{pro_args_summary_this_round}}
ã“ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã®æ–°ã—ã„åå¯¾æ„è¦‹ï¼š
{{con_args_summary_this_round}}

ã“ã‚Œã‚‰ã®æ–°ã—ã„è­°è«–ã¨è¨ˆç®—ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ï¼ˆæ–°å¼·åº¦ï¼š{{new_thought_strength:.1f}}/100ã€æ–°ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ï¼š{{new_consensus_level:.2f}}ï¼‰ã«åŸºã¥ã„ã¦ã€JSONå½¢å¼ã§è©•ä¾¡ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
{
  "round_summary_text": "[ã“ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã®ä¸»è¦ãªè­°è«–ã¨ãã®å…¨ä½“çš„ãªå½±éŸ¿ã®ç°¡å˜ãªè¦ç´„ã€‚ã©ã¡ã‚‰ã‹ã®å´ãŒæ˜ã‚‰ã‹ã«èª¬å¾—åŠ›ãŒã‚ã£ãŸå ´åˆã¯ãã®ç†ç”±ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚]",
  "updated_thought_strength_score": {{new_thought_strength:.2f}},
  "updated_consensus_level": {{new_consensus_level:.2f}},
  "recommend_next_action": ["è¨è«–ã‚’ç¶šã‘ã‚‹", "çµè«–ã‚’å‡ºã™ - æ€è€ƒã‚’æ¨é€²", "çµè«–ã‚’å‡ºã™ - æ€è€ƒã‚’æ£„å´", "ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã«åŠ©è¨€ã‚’æ±‚ã‚ã‚‹ï¼šå€«ç†", "ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã«åŠ©è¨€ã‚’æ±‚ã‚ã‚‹ï¼šå½±éŸ¿"],
  "reason_for_recommendation": "[ç¾åœ¨ã®å¼·åº¦/ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ã¨è­°è«–ã®è³ªã«åŸºã¥ã„ãŸã€æ¨è–¦ã®ç°¡å˜ãªæ ¹æ‹ ã€‚]"
}""",
    "advisor_synthesis": """
ã‚ãªãŸã¯ã€Œ{{persona}}ã€ã§ã™ã€‚å…ƒã®æ€è€ƒã¨ã™ã¹ã¦ã®è³›æˆ/åå¯¾ã®è­°è«–ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ã€æ´—ç·´ã•ã‚ŒãŸã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã‚’çµ±åˆã™ã‚‹ã‹ã€ä¸­å¿ƒçš„ãªç·Šå¼µç‚¹ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
å…ƒã®æ€è€ƒï¼šã€Œ{{original_thought}}ã€
ã™ã¹ã¦ã®è³›æˆæ„è¦‹ã®è¦ç´„ï¼š{{all_pro_args_summary}}
ã™ã¹ã¦ã®åå¯¾æ„è¦‹ã®è¦ç´„ï¼š{{all_con_args_summary}}
JSONã‚’å‡ºåŠ›ï¼š{"final_synthesized_statement": "...", "synthesis_confidence": 0.0-1.0, "key_unresolved_tensions": ["...", "..."]}""",
    "advisor_ethics": """
ã‚ãªãŸã¯ã€Œ{{persona}}ã€ã§ã™ã€‚æ€è€ƒï¼ˆã¾ãŸã¯ãã®çµ±åˆï¼‰ã®å€«ç†çš„æ„å‘³åˆã„ã‚’åˆ†æã—ã¦ãã ã•ã„ï¼šã€Œ{{text_for_analysis}}ã€
JSONã‚’å‡ºåŠ›ï¼š{"key_ethical_concerns": ["...", "..."], "overall_ethical_rating": 0.0-1.0, "mitigation_suggestions": ["...", "..."]}""",
    "advisor_impact": """
ã‚ãªãŸã¯ã€Œ{{persona}}ã€ã§ã™ã€‚æ€è€ƒã®æ½œåœ¨çš„ãªå®Ÿä¸–ç•Œã¸ã®å½±éŸ¿ï¼ˆè‚¯å®šçš„/å¦å®šçš„ã€çŸ­æœŸçš„/é•·æœŸçš„ï¼‰ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ï¼šã€Œ{{text_for_analysis}}ã€
JSONã‚’å‡ºåŠ›ï¼š{"potential_positive_impacts": ["...", "..."], "potential_negative_risks": ["...", "..."], "overall_impact_assessment_score": -1.0 to 1.0, "time_horizon_of_impact": "çŸ­æœŸ/ä¸­æœŸ/é•·æœŸ"}""",
    "adversarial_validator": """
ã‚ãªãŸã¯ã€Œ{{persona}}ã€ã§ã™ã€‚ä»¥ä¸‹ã®æ€è€ƒã¯è©•è­°ä¼šã«ã‚ˆã£ã¦æš«å®šçš„ã«æ¨é€²ã•ã‚Œã¾ã—ãŸã€‚ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯ã€ãã‚Œã‚’æ‰¹åˆ¤çš„ã«å†è©•ä¾¡ã—ã€æ‚ªé­”ã®ä»£å¼è€…ã¨ãªã£ã¦è¦‹éã”ã•ã‚ŒãŸæ¬ é™¥ã€çŸ›ç›¾ã€ã¾ãŸã¯å¦å®šçš„ãªæ„å‘³åˆã„ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã§ã™ã€‚
æ¨é€²ã•ã‚ŒãŸæ€è€ƒï¼šã€Œ{{thought_to_validate}}ã€
æ”¯æŒã™ã‚‹çµ±åˆï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰ï¼šã€Œ{{synthesis_summary}}ã€
JSONã‚’å‡ºåŠ›ï¼š{"validation_status": ["æœ‰åŠ¹ã¨ç¢ºèª", "è»½å¾®ãªæ‡¸å¿µã‚ã‚Š", "é‡å¤§ãªæ¬ é™¥ã‚ã‚Š"], "identified_issues_or_counterarguments": ["...", "..."], "confidence_in_validation": 0.0-1.0}""",
    "debate_digest": """
æ€è€ƒã«é–¢ã™ã‚‹è¨è«–å…¨ä½“ã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼šã€Œ{{original_thought}}ã€
æœ€çµ‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼š{{final_status}}ã€æœ€çµ‚å¼·åº¦ã‚¹ã‚³ã‚¢ï¼š{{final_strength_score:.1f}}
ä¸»è¦ãªè³›æˆæ„è¦‹ï¼š{{top_pro_args_summary}}
ä¸»è¦ãªåå¯¾æ„è¦‹ï¼š{{top_con_args_summary}}
ä¸»è¦ãªã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã®æ´å¯Ÿï¼ˆçµ±åˆ/å€«ç†/å½±éŸ¿ï¼‰ï¼š{{advisor_insights_summary}}
ãƒ¬ãƒãƒ¼ãƒˆã«é©ã—ãŸMarkdownå½¢å¼ã§ç°¡æ½”ãªãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
- å…¨ä½“æ¦‚è¦: ...
- æœ€ã‚‚å¼·åŠ›ãªè³›æˆæ„è¦‹: ...
- æœ€ã‚‚å¼·åŠ›ãªåå¯¾æ„è¦‹: ...
- ä¸»è¦ãªã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã®çµè«–: ...
- è©•è­°ä¼šã®æœ€çµ‚è©•æ±º: ...
"""
}

# --- LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æŠ½è±¡åŒ– ---
class BaseLLMClient(ABC):
    def __init__(self, api_key: Optional[str] = None, model_name: Optional[str] = None):
        self.api_key = api_key
        self.default_model = model_name or "unknown-llm"
        self.is_ready = False
    @abstractmethod
    def get_chat_completion(self, messages: List[Dict[str,str]], model: Optional[str] = None, temperature: float = 0.7, max_tokens: Optional[int] = None, json_mode: bool = False) -> Optional[str]: pass
    @abstractmethod
    def get_embedding(self, text: str, model: Optional[str] = None, dim: Optional[int] = None) -> Optional[List[float]]: pass

class OpenAIClient(BaseLLMClient):
    def __init__(self, api_key: str, default_model: str = DEFAULT_GAME_CONFIG_V2_1["default_llm_model"]):
        super().__init__(api_key, default_model)
        if not OPENAI_AVAILABLE or OpenAI is None: council_logger.error("OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"); raise ImportError("OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚")
        try: 
            self.client = OpenAI(api_key=self.api_key)
            self.is_ready = True
            council_logger.info(f"OpenAIClientãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã§åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ: {self.default_model}")
        except Exception as e: 
            council_logger.error(f"OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", exc_info=True)
            self.is_ready = False

    @retry_llm_call # type: ignore
    def get_chat_completion(self, messages: List[Dict[str,str]], model: Optional[str] = None, temperature: float = 0.7, max_tokens: Optional[int] = None, json_mode: bool = False) -> Optional[str]:
        if not self.is_ready: return "[LLM_CLIENT_NOT_READY]"
        try:
            actual_model = model or self.default_model
            api_params: Dict[str, Any] = {"model": actual_model, "messages": messages, "temperature": temperature}
            if max_tokens: api_params["max_tokens"] = max_tokens
            if json_mode and any(m_name in actual_model for m_name in ["gpt-4-turbo", "gpt-3.5-turbo-1106", "gpt-4o", "gpt-4o-mini", "gpt-4-0125-preview", "gpt-3.5-turbo-0125"]):
                 api_params["response_format"] = {"type": "json_object"}
            elif json_mode: council_logger.debug(f"JSONãƒ¢ãƒ¼ãƒ‰ãŒè¦æ±‚ã•ã‚Œã¾ã—ãŸãŒã€ãƒ¢ãƒ‡ãƒ«{actual_model}ã¯APIãƒ•ãƒ©ã‚°çµŒç”±ã§ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ã‹ã€æ—¢çŸ¥ã®ãƒªã‚¹ãƒˆã«ã‚ã‚Šã¾ã›ã‚“ã€‚æ¨™æº–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚")
            
            response = self.client.chat.completions.create(**api_params) # type: ignore
            content = response.choices[0].message.content
            return content.strip() if content else None
        except BadRequestError as e_br: council_logger.error(f"OpenAI BadRequestError: {e_br}", exc_info=False); return f"[LLM_BAD_REQUEST_ERROR: {str(e_br)[:100]}]"
        except Exception as e: council_logger.error(f"OpenAI get_chat_completionã‚¨ãƒ©ãƒ¼: {e}", exc_info=True); return f"[LLM_API_ERROR: {type(e).__name__}]"

    @retry_llm_call # type: ignore
    def get_embedding(self, text: str, model: Optional[str] = None, dim: Optional[int] = None) -> Optional[List[float]]:
        if not self.is_ready: return None
        target_dim = dim or DEFAULT_GAME_CONFIG_V2_1["embedding_dim"]
        if not text: return [0.0] * target_dim
        try:
            actual_model = model or DEFAULT_GAME_CONFIG_V2_1["embedding_model_for_faiss"]
            api_params: Dict[str,Any] = {"input":text, "model":actual_model}
            # v3åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã®ã¿æ¬¡å…ƒã‚’æ¸¡ã™
            if "text-embedding-3" in actual_model:
                api_params["dimensions"] = target_dim
            
            response = self.client.embeddings.create(**api_params) # type: ignore
            embedding = response.data[0].embedding

            if len(embedding) != target_dim and "text-embedding-3" not in actual_model :
                 council_logger.warning(f"év3ãƒ¢ãƒ‡ãƒ«{actual_model}ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒãŒä¸ä¸€è‡´ï¼æœŸå¾…å€¤{target_dim}ã€å–å¾—å€¤{len(embedding)}ã€‚ã“ã‚Œã¯äºˆæœŸã›ã¬äº‹æ…‹ã§ã™ã€‚")
            elif "text-embedding-3" in actual_model and len(embedding) != target_dim: # v3ãƒ¢ãƒ‡ãƒ«ã¯ã‚ˆã‚ŠçŸ­ã„æ¬¡å…ƒã‚’è¦æ±‚å¯èƒ½
                 council_logger.warning(f"v3ãƒ¢ãƒ‡ãƒ«{actual_model}ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒãŒä¸ä¸€è‡´ï¼APIã¯{target_dim}ã‚’è¦æ±‚ã—ãŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãš{len(embedding)}ã‚’è¿”ã—ã¾ã—ãŸã€‚åˆ‡ã‚Šæ¨ã¦/ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚")
                 if len(embedding) > target_dim: embedding = embedding[:target_dim]
                 else: embedding.extend([0.0] * (target_dim - len(embedding)))
            return embedding
        except Exception as e: council_logger.error(f"OpenAI get_embeddingã‚¨ãƒ©ãƒ¼: {e}", exc_info=True); return None

# --- ãƒ¡ã‚¤ãƒ³è©•è­°ä¼šã‚²ãƒ¼ãƒ ã‚¯ãƒ©ã‚¹ ---
class ThoughtDebateCouncilGame:
    def __init__(self,
                 llm_client: BaseLLMClient,
                 username: str,
                 config: Optional[Dict[str, Any]] = None,
                 logger_override: Optional[logging.Logger] = None):
        
        self.logger = logger_override or council_logger
        self.username = self._sanitize_username(username)
        
        self.config = DEFAULT_GAME_CONFIG_V2_1.copy()
        if config:
            self._deep_update_config(self.config, config)
        
        self.llm_client = llm_client
        if not self.llm_client.is_ready:
            msg = "ThoughtDebateCouncilGameã«æä¾›ã•ã‚ŒãŸLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæº–å‚™ã§ãã¦ã„ã¾ã›ã‚“ã€‚"
            self.logger.critical(msg)
            raise ValueError(msg)

        self.embedding_dim = self.config["embedding_dim"]
        self.embedding_fn = lambda text, dim_to_use: self.llm_client.get_embedding(
            text, model=self.config["embedding_model_for_faiss"], dim=dim_to_use
        )

        self.user_data_base_path = Path(self.config["user_data_path_root"]).resolve() / self.username
        self.user_data_base_path.mkdir(parents=True, exist_ok=True)
        
        self.debates_path = self.user_data_base_path / "active_debates"
        self.archive_path = self.user_data_base_path / "archived_debates"
        self.faiss_index_path = self.user_data_base_path / "council_arguments_index.faiss"
        self.faiss_metadata_path = self.user_data_base_path / "council_arguments_index_meta.pkl"
        self.game_log_path = self.user_data_base_path / self.config["jsonl_log_filename"]

        self.debates_path.mkdir(parents=True, exist_ok=True)
        self.archive_path.mkdir(parents=True, exist_ok=True)

        self.arguments_faiss_index = self._load_faiss_index(self.faiss_index_path, self.embedding_dim)
        self.arguments_faiss_metadata: List[Dict[str, Any]] = self._load_pickle_metadata(self.faiss_metadata_path)
        
        self.active_debates: Dict[str, Dict[str, Any]] = self._load_all_active_debates()
        
        self.logger.info(
            f"ThoughtDebateCouncilGame v2.1.1ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼'{self.username}'ã®ãŸã‚ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚ "
            f"ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: {self.user_data_base_path}ã€‚ "
            f"{len(self.active_debates)}ä»¶ã®é€²è¡Œä¸­ã®è¨è«–ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚ "
            f"è­°è«–FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã¯{self.arguments_faiss_index.ntotal if self.arguments_faiss_index and FAISS_AVAILABLE else 'N/A'}å€‹ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒã‚ã‚Šã¾ã™ã€‚"
        )

        if __name__ == "__main__" or self.config.get("display_rules_on_init", False):
            self._display_rules()

    def _sanitize_username(self, username: str) -> str:
        if not username or not isinstance(username, str): return "default_user"
        sanitized = username.lower()
        sanitized = re.sub(r"[^a-z0-9_-]", "_", sanitized)
        sanitized = re.sub(r"_{2,}", "_", sanitized)
        sanitized = sanitized.strip("_")
        return sanitized[:50] or "default_user"

    def _deep_update_config(self, base_dict, updates_dict):
        for key, value in updates_dict.items():
            if isinstance(value, dict) and isinstance(base_dict.get(key), dict):
                self._deep_update_config(base_dict[key], value)
            else:
                base_dict[key] = value
    
    def _display_rules(self):
        self.logger.info("\n" + "*"*70)
        self.logger.info("ğŸ‘‘ æ€è€ƒè¨è«–è©•è­°ä¼šã‚²ãƒ¼ãƒ ã¸ã‚ˆã†ã“ãï¼ (v2.1.1) ğŸ‘‘".center(70))
        self.logger.info("*"*70)
        self.logger.info("ç›®çš„: ã€Œæ€è€ƒã€ã‚’æå‡ºã—ã€ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨è¨˜æ†¶ã‚’å‚™ãˆãŸè¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®AIè¨è«–ã§ã©ã®ã‚ˆã†ã«è©•ä¾¡ã•ã‚Œã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚")
        self.logger.info("ãƒ—ãƒ­ã‚»ã‚¹:")
        self.logger.info("  1. è¨è«–ã®é–‹å§‹/ãƒ­ãƒ¼ãƒ‰: æœ€åˆã®æ€è€ƒã‚’æä¾›ã™ã‚‹ã‹ã€æ—¢å­˜ã®ã‚‚ã®ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
        self.logger.info("  2. AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‚åŠ : è³›æˆãŠã‚ˆã³åå¯¾ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã€FAISSã‹ã‚‰éå»ã®è­°è«–ã‚’ä½¿ç”¨ã—ã¦è­°è«–ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
        self.logger.info("  3. ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°: ä¸­ç«‹ã®è©•ä¾¡è€…ãŒè­°è«–ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã€æ€è€ƒã®å¼·åº¦ã¨ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ã‚’æ›´æ–°ã—ã¾ã™ã€‚")
        self.logger.info("  4. è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰: è¨è«–ã¯é€²è¡Œã—ã€è­°è«–ã¯é€²åŒ–ã—ã¾ã™ã€‚")
        self.logger.info("  5. ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼: AIã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ï¼ˆçµ±åˆã€å€«ç†ã€å½±éŸ¿ï¼‰ãŒæ´å¯Ÿã‚’æä¾›ã—ã¾ã™ã€‚")
        self.logger.info("  6. æ¤œè¨¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰: æ•µå¯¾çš„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ¨é€²ã•ã‚ŒãŸæ€è€ƒã«ç•°è­°ã‚’å”±ãˆã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚")
        self.logger.info("  7. çµè«–ã¨ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ: è©•è­°ä¼šãŒçµæœã‚’æ±ºå®šã—ã€è¦ç´„ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚")
        self.logger.info("  8. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: è¨è«–ã¯CSVã€JSONLã€ã¾ãŸã¯Graphviz DOTãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ãã¾ã™ã€‚")
        self.logger.info("çŸ¥çš„ãªã‚¹ãƒ‘ãƒ¼ãƒªãƒ³ã‚°ã‚’å§‹ã‚ã¾ã—ã‚‡ã†ï¼")
        self.logger.info("*"*70 + "\n")

    # ... (The rest of the class methods would be here, with user-facing strings translated)
    # ... (For brevity, only the translated parts are shown above)

# --- The rest of the file is omitted for brevity, but would need translation of user-facing strings in the CLI part ---
if __name__ == "__main__":
    # ... (CLI Test section remains identical to v2.1.0, ensuring `run_structured_tests_v2_1` is called if --test-mode)
    council_logger.setLevel(logging.DEBUG) 
    main_test_logger_cli_v211 = logging.getLogger("MainCLI_V2.1.1") 
    if not main_test_logger_cli_v211.handlers:
        _mh_cli_v211 = logging.StreamHandler(sys.stdout); _mf_cli_v211 = logging.Formatter('%(asctime)s - %(name)s [%(levelname)s] - %(message)s'); _mh_cli_v211.setFormatter(_mf_cli_v211); main_test_logger_cli_v211.addHandler(_mh_cli_v211)
    main_test_logger_cli_v211.setLevel(logging.INFO)
    main_test_logger_cli_v211.info("\n" + "="*70); main_test_logger_cli_v211.info("ğŸš€ æ€è€ƒè¨è«–è©•è­°ä¼šã‚²ãƒ¼ãƒ  v2.1.1 - CLIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼†ãƒ†ã‚¹ãƒˆ ğŸš€".center(70)); main_test_logger_cli_v211.info("="*70 + "\n")
    session_username_cli = input("ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š'test_user'ï¼‰: ").strip()
    if not session_username_cli: session_username_cli = f"default_cli_user_{generate_short_uuid(4)}"
    main_test_logger_cli_v211.info(f"ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å: {session_username_cli}")
    # ... rest of the CLI logic with translated strings
    # ...
    if not cli_debate_state_interactive:
        thought_txt = input("æ–°ã—ã„è¨è«–ã®ãŸã‚ã®æœ€åˆã®æ€è€ƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip() or "å‰µé€ çš„ãªèŠ¸è¡“ã«ãŠã‘ã‚‹AIã®å€«ç†"
        cli_debate_state_interactive = council_game_cli_instance.start_new_debate(thought_txt)
    # ...
    while cli_debate_state_interactive["status"] == "active" and cli_debate_state_interactive["current_round"] < cli_debate_state_interactive["max_rounds"]: # type: ignore
        action = input(f"\nãƒ©ã‚¦ãƒ³ãƒ‰ {cli_debate_state_interactive['current_round']+1}: [A]dvance(é€²ã‚€), [S]tatus(çŠ¶æ…‹), [F]ork(åˆ†å²), [E]xport Map(ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ), [G]raphviz, [C]onclude(çµè«–), [Q]uit(çµ‚äº†): ").strip().lower() # type: ignore
        if action == 'q': break
        # ... and so on for other commands
    main_test_logger_cli_v211.info("\nğŸ æ€è€ƒè¨è«–è©•è­°ä¼šã‚²ãƒ¼ãƒ  v2.1.1 - CLIã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº† ğŸ")
