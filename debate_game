# thought_debate_council_game_v2_1_1.py
from __future__ import annotations # 型ヒントの遅延評価を有効にする

"""
👑 自律思考討論カーネル - v2.1.1 👑
   作成者: nobody@nowhere.net (自己完結型の探求)

ユーザーが提出した思考について、AI駆動の討論を調整するためのスタンドアロンアプリケーション。
このモジュールは自己完結型で設計されており、AIエージェントとの構造化された議論を通じて
アイデアを探求するためのリッチな環境を提供します。

----------------------------------------------------------------------------------------------------
🚀 このモジュールの使い方 (スタンドアロンCLIまたはライブラリとして) 🚀

**I. コアコンセプト:**
   このスクリプトは、AIエージェントが異なる役割（賛成、反対、中立評価者、アドバイザー）を担い、
   与えられた「思考」を複数のラウンドにわたって分析・評価する「討論評議会」をシミュレートします。
   目標は、思考の強度を判断し、コンセンサスを達成し、その意味合いを探求することです。

**II. 前提条件とインストール:**

   1. Python 3.9+ を推奨 (`from __future__ import annotations`が標準動作のため)。
   2. 必要なライブラリをインストール:
      pip install openai numpy faiss-cpu tenacity pydantic python-dotenv
      (互換性のあるNVIDIA GPUとCUDAセットアップがある場合は`faiss-cpu`の代わりに`faiss-gpu`を使用)
   3. グラフ視覚化エクスポートのためのオプション:
      pip install graphviz
      (Graphvizシステムツールもインストールされている必要があります: https://graphviz.org/download/)

**III. セットアップ - APIキー (重要！):**

   このアプリケーションは、LLMと機能するためにOpenAI APIキーが必要です。
   - **推奨方法 (環境変数):**
     `OPENAI_API_KEY`という名前の環境変数に有効なキーを設定します。
     - Windows (コマンドプロンプト): `set OPENAI_API_KEY=sk-YourActualOpenApiKeyHere`
     - Windows (PowerShell):   `$env:OPENAI_API_KEY="sk-YourActualOpenApiKeyHere"`
     - Linux/macOS (Bash/Zsh): `export OPENAI_API_KEY="sk-YourActualOpenApiKeyHere"`
   - **代替方法 (.envファイル):**
     このスクリプトと同じディレクトリ（またはプロジェクトルート）に`.env`という名前のファイルを作成し、
     `OPENAI_API_KEY="sk-YourActualOpenApiKeyHere"`という行を追加します。
     (`python-dotenv`がインストールされている必要があります。この方法を使用する場合は、スクリプトの先頭に`from dotenv import load_dotenv; load_dotenv()`を追加してください)。

**IV. インタラクティブCLIゲームの実行:**

   1. このスクリプトが含まれるディレクトリに移動します。
   2. 実行: `python thought_debate_council_game_v2_1_1.py` (またはあなたのファイル名)
   3. **ユーザー名を入力:** ユーザー名の入力を求められます。これは、あなたの討論データを
      別のフォルダ（例：`./council_cli_user_data_v2_1/your_username/`）に整理するために使用されます。
   4. **プロンプトに従う:**
      - 思考/ステートメントを入力して新しい討論を開始します。
      - そのユーザー名で以前のセッションがある場合は、既存の進行中の討論をロードします。
      - ラウンドを進めたり、ステータスを表示したり、討論をフォークしたり、データをエクスポートしたり、結論付けたりします。

   **インタラクティブコマンド:**
     - `[A]dvance` (または単にEnterキーを押す): 次の討論ラウンドに進みます。
     - `[S]tatus`: 討論の現在の詳細な状態を表示します。
     - `[F]ork`: 現在の討論に基づいて、変更された思考で新しい討論を作成します。
     - `[E]xport Map`: 議論をCSVまたはJSONLにエクスポートします。
     - `[G]raphviz`: 討論の流れをGraphviz DOTファイルとしてエクスポートします。
     - `[C]onclude`: 現在の討論を終了し、最終的なアドバイザーを実行します。
     - `[Q]uit`: インタラクティブセッションを終了します。

**V. 構造化テストモードでの実行:**

   このモードでは、期待される結果に対して討論シナリオの自動テストが可能です。

   1. **テスト計画JSONファイルを作成:**
      (`run_structured_tests_v2_1`関数のドキュメント内の例の構造を参照)
      `initial_thought`と`expected_outcome`を持つ`debate_test_cases`を定義します。

   2. **実行:**
      python thought_debate_council_game_v2_1_1.py --test-mode /path/to/your_test_plan.json

   3. **出力を確認:** スクリプトは各テストケースのPASS/FAILをログに記録し、
      テストされた各討論のエクスポート（CSVマップ、DOTグラフ）を保存します。

**VI. 独自のPythonプロジェクトでライブラリとして使用:**

   from thought_debate_council_game_v2_1_1 import ThoughtDebateCouncilGame, OpenAIClient, DEFAULT_GAME_CONFIG_V2_1

   # 1. LLMクライアントを初期化
   my_api_key = os.getenv("MY_PROJECT_OPENAI_KEY") # キーを安全に管理
   llm_client = OpenAIClient(api_key=my_api_key)

   # 2. 評議会ゲームを設定
   council_config = DEFAULT_GAME_CONFIG_V2_1.copy()
   council_config["user_data_path_root"] = "./my_app_council_data" # ユーザーデータはアプリ相対で保存
   
   council_game = ThoughtDebateCouncilGame(
       llm_client=llm_client,
       username="my_app_user_session_1", 
       config=council_config
   )

   # 3. 対話 (例: 討論を開始して1ラウンド実行)
   debate1 = council_game.start_new_debate("科学的発見におけるAIの役割")
   if debate1['status'] == 'active':
       debate1 = council_game.advance_debate_round(debate1['debate_id'])
   print(json.dumps(debate1, indent=2, default=str))

**VII. 主な機能と設計 (v2.1.1):**
    - ユーザー関連データ: 討論/ログはユーザー名ごとに保存。
    - 永続的な状態: 進行中の討論はロード/セーブされ、結論が出た討論はアーカイブされる。
    - 動的スコアリング: 思考の強度とコンセンサスが進化する。
    - 議論FAISS: 個々の議論をメタデータと共に保存。
    - LLM抽象化: `BaseLLMClient`インターフェース（OpenAIClient提供）。
    - 設定可能: モデル、ペルソナ、しきい値、パスを辞書で設定。
    - 埋め込みプロンプトライブラリ: 明確なLLM指示のため。
    - パーソナリティドリフト: AIエージェントの温度のオプションの変動。
    - 高度なアドバイザーと敵対的検証。
    - データエクスポート: CSV/JSONLの議論マップ、Graphviz DOTの討論グラフ。
    - 構造化テスト: JSONプランによる自動テスト。
    - 自己完結型: 一般的なライブラリ依存関係を持つ単一のPythonファイル。
----------------------------------------------------------------------------------------------------
"""

import os
import sys
import json
import hashlib
import pickle
import uuid
import time
import logging
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Callable, Union
from abc import ABC, abstractmethod
import random
import csv
import statistics # IMPORTED

# --- 依存関係のチェックとインポート ---
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError: NUMPY_AVAILABLE = False; np = None
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError: FAISS_AVAILABLE = False; faiss = None
try:
    from openai import OpenAI, APIError as OpenAIAPIError, RateLimitError, APIConnectionError, BadRequestError
    OPENAI_AVAILABLE = True
except ImportError: OPENAI_AVAILABLE = False; OpenAI = OpenAIAPIError = RateLimitError = APIConnectionError = BadRequestError = None # type: ignore
try:
    import tenacity
    TENACITY_AVAILABLE = True
    retry_llm_call = tenacity.retry(
        wait=tenacity.wait_exponential(multiplier=1, min=2, max=30),
        stop=tenacity.stop_after_attempt(3),
        retry=tenacity.retry_if_exception_type((OpenAIAPIError, RateLimitError, APIConnectionError)) if OPENAI_AVAILABLE and OpenAIAPIError else None, # type: ignore
        before_sleep=tenacity.before_sleep_log(logging.getLogger("ThoughtDebateCouncilGameV2.1.1"), logging.WARNING)
    )
except ImportError:
    TENACITY_AVAILABLE = False
    def retry_llm_call(func): return func # type: ignore
    logging.getLogger("ThoughtDebateCouncilGameV2.1.1").warning("Tenacityライブラリが見つかりません。LLM呼び出しのリトライロジックは無効になります。")

try:
    import graphviz # .dotエクスポートのためのオプション
    GRAPHVIZ_AVAILABLE = True
except ImportError:
    GRAPHVIZ_AVAILABLE = False
    logging.getLogger("ThoughtDebateCouncilGameV2.1.1").info("Graphvizライブラリが見つかりません。DOTファイルのエクスポートは無効になります。")


# --- ロガーのセットアップ ---
council_logger = logging.getLogger("ThoughtDebateCouncilGameV2.1.1")
if not council_logger.handlers:
    _handler = logging.StreamHandler(sys.stdout)
    _formatter = logging.Formatter('%(asctime)s - %(name)s [%(levelname)s] - [%(filename)s:%(lineno)d] - %(message)s')
    _handler.setFormatter(_formatter)
    council_logger.addHandler(_handler)
    council_logger.setLevel(os.getenv("COUNCIL_GAME_LOG_LEVEL", "INFO").upper())


# --- 設定と定数 ---
DEFAULT_GAME_CONFIG_V2_1 = {
    "default_llm_model": "gpt-4o-mini",
    "embedding_model_for_faiss": "text-embedding-3-small",
    "embedding_dim": 1536,
    "debate_rounds_max": 3,
    "agents_per_side_per_round": 1,
    "user_data_path_root": "./council_user_data_v2_1",
    "log_to_jsonl": True,
    "jsonl_log_filename": "council_game_events.jsonl",
    "debate_agents": {
        "pro": {"persona": "先見の明ある支持者", "temperature": 0.72, "max_tokens": 350},
        "con": {"persona": "現実的な懐疑論者", "temperature": 0.68, "max_tokens": 350},
        "neutral_evaluator": {"persona": "首席裁定官", "temperature": 0.3, "max_tokens": 450},
        "argument_recombiner": {"persona": "創造的統合者（記憶）", "temperature": 0.6, "max_tokens": 350},
        "adversarial_validator": {"persona": "懐疑的なレッドチーム", "temperature": 0.7, "max_tokens": 400}
    },
    "advisor_personas": {
        "synthesis": {"persona":"全体的インテグレーター", "temperature":0.55, "max_tokens":400},
        "ethics": {"persona":"倫理の羅針盤", "temperature":0.4, "max_tokens":350},
        "impact": {"persona":"先見の明あるストラテジスト", "temperature":0.5, "max_tokens":350},
        "fallacy": {"persona":"論理の番人", "temperature":0.3, "max_tokens":300},
        "meta_observer": {"persona":"プロセス監査官", "temperature":0.3, "max_tokens":300}
    },
    "initial_thought_score": 50.0,
    "persuasiveness_score_impact_factor": 0.1,
    "consensus_threshold_promote": 70.0,
    "consensus_threshold_reject": 35.0,
    "max_argument_length_for_prompt": 150,
    "faiss_search_top_k_for_recombiner": 3,
    "enable_personality_drift": True,
    "temperature_drift_range": (-0.05, 0.05),
    "max_active_debates_in_memory": 20
}

# --- 埋め込みミニプロンプトライブラリ ---
PROMPT_TEMPLATES = {
    "generate_argument": """
あなたは討論のラウンド{{round_num}}における「{{persona}}」というペルソナを持つAIエージェントです。
討論されている元の思考は次のとおりです：「{{original_thought}}」
あなたの側（{{agent_type.upper()}}）の既存の議論（もしあれば、最新のものから順に）：
{{existing_args_summary}}

あなたのタスクは、新しく、簡潔で、説得力のある{{agent_type.upper()}}の議論を生成することです。
明確な点に焦点を当てるか、以前の議論を批判的に発展させてください。強力な論理と新規性を目指してください。
厳密にJSON形式で、これらの正確なキーで出力してください：
{
  "argument_text": "[あなたの簡潔な議論、最大2-3文の洞察に富んだ文。ポイントの繰り返しは避けてください。]",
  "persuasiveness_score": "[浮動小数点数：この特定の議論が新規性と影響を考慮してどれだけ説得力があるか、0.1から1.0の自己評価スコア]",
  "keywords": ["[あなたの", "議論から", "3-5個の", "主要な", "用語のリスト"]
}""",
    "evaluate_round": """
あなたは「{{persona}}」です。討論の議題は：「{{original_thought}}」
現在の思考の強度：{{thought_strength:.1f}}/100。コンセンサスレベル：{{consensus_level:.2f}}。
これはラウンド{{current_round}}です。
このラウンドの新しい賛成意見：
{{pro_args_summary_this_round}}
このラウンドの新しい反対意見：
{{con_args_summary_this_round}}

これらの新しい議論と計算されたスコア（新強度：{{new_thought_strength:.1f}}/100、新コンセンサス：{{new_consensus_level:.2f}}）に基づいて、JSON形式で評価を提供してください：
{
  "round_summary_text": "[このラウンドの主要な議論とその全体的な影響の簡単な要約。どちらかの側が明らかに説得力があった場合はその理由を述べてください。]",
  "updated_thought_strength_score": {{new_thought_strength:.2f}},
  "updated_consensus_level": {{new_consensus_level:.2f}},
  "recommend_next_action": ["討論を続ける", "結論を出す - 思考を推進", "結論を出す - 思考を棄却", "アドバイザーに助言を求める：倫理", "アドバイザーに助言を求める：影響"],
  "reason_for_recommendation": "[現在の強度/コンセンサスと議論の質に基づいた、推薦の簡単な根拠。]"
}""",
    "advisor_synthesis": """
あなたは「{{persona}}」です。元の思考とすべての賛成/反対の議論をレビューして、洗練されたステートメントを統合するか、中心的な緊張点を特定してください。
元の思考：「{{original_thought}}」
すべての賛成意見の要約：{{all_pro_args_summary}}
すべての反対意見の要約：{{all_con_args_summary}}
JSONを出力：{"final_synthesized_statement": "...", "synthesis_confidence": 0.0-1.0, "key_unresolved_tensions": ["...", "..."]}""",
    "advisor_ethics": """
あなたは「{{persona}}」です。思考（またはその統合）の倫理的意味合いを分析してください：「{{text_for_analysis}}」
JSONを出力：{"key_ethical_concerns": ["...", "..."], "overall_ethical_rating": 0.0-1.0, "mitigation_suggestions": ["...", "..."]}""",
    "advisor_impact": """
あなたは「{{persona}}」です。思考の潜在的な実世界への影響（肯定的/否定的、短期的/長期的）を評価してください：「{{text_for_analysis}}」
JSONを出力：{"potential_positive_impacts": ["...", "..."], "potential_negative_risks": ["...", "..."], "overall_impact_assessment_score": -1.0 to 1.0, "time_horizon_of_impact": "短期/中期/長期"}""",
    "adversarial_validator": """
あなたは「{{persona}}」です。以下の思考は評議会によって暫定的に推進されました。あなたのタスクは、それを批判的に再評価し、悪魔の代弁者となって見過ごされた欠陥、矛盾、または否定的な意味合いを見つけることです。
推進された思考：「{{thought_to_validate}}」
支持する統合（もしあれば）：「{{synthesis_summary}}」
JSONを出力：{"validation_status": ["有効と確認", "軽微な懸念あり", "重大な欠陥あり"], "identified_issues_or_counterarguments": ["...", "..."], "confidence_in_validation": 0.0-1.0}""",
    "debate_digest": """
思考に関する討論全体を要約してください：「{{original_thought}}」
最終ステータス：{{final_status}}、最終強度スコア：{{final_strength_score:.1f}}
主要な賛成意見：{{top_pro_args_summary}}
主要な反対意見：{{top_con_args_summary}}
主要なアドバイザーの洞察（統合/倫理/影響）：{{advisor_insights_summary}}
レポートに適したMarkdown形式で簡潔なダイジェストを出力してください：
- 全体概要: ...
- 最も強力な賛成意見: ...
- 最も強力な反対意見: ...
- 主要なアドバイザーの結論: ...
- 評議会の最終評決: ...
"""
}

# --- LLMクライアントの抽象化 ---
class BaseLLMClient(ABC):
    def __init__(self, api_key: Optional[str] = None, model_name: Optional[str] = None):
        self.api_key = api_key
        self.default_model = model_name or "unknown-llm"
        self.is_ready = False
    @abstractmethod
    def get_chat_completion(self, messages: List[Dict[str,str]], model: Optional[str] = None, temperature: float = 0.7, max_tokens: Optional[int] = None, json_mode: bool = False) -> Optional[str]: pass
    @abstractmethod
    def get_embedding(self, text: str, model: Optional[str] = None, dim: Optional[int] = None) -> Optional[List[float]]: pass

class OpenAIClient(BaseLLMClient):
    def __init__(self, api_key: str, default_model: str = DEFAULT_GAME_CONFIG_V2_1["default_llm_model"]):
        super().__init__(api_key, default_model)
        if not OPENAI_AVAILABLE or OpenAI is None: council_logger.error("OpenAIライブラリが利用できません。"); raise ImportError("OpenAIライブラリが必要です。")
        try: 
            self.client = OpenAI(api_key=self.api_key)
            self.is_ready = True
            council_logger.info(f"OpenAIClientがデフォルトモデルで初期化されました: {self.default_model}")
        except Exception as e: 
            council_logger.error(f"OpenAIクライアントの初期化に失敗しました: {e}", exc_info=True)
            self.is_ready = False

    @retry_llm_call # type: ignore
    def get_chat_completion(self, messages: List[Dict[str,str]], model: Optional[str] = None, temperature: float = 0.7, max_tokens: Optional[int] = None, json_mode: bool = False) -> Optional[str]:
        if not self.is_ready: return "[LLM_CLIENT_NOT_READY]"
        try:
            actual_model = model or self.default_model
            api_params: Dict[str, Any] = {"model": actual_model, "messages": messages, "temperature": temperature}
            if max_tokens: api_params["max_tokens"] = max_tokens
            if json_mode and any(m_name in actual_model for m_name in ["gpt-4-turbo", "gpt-3.5-turbo-1106", "gpt-4o", "gpt-4o-mini", "gpt-4-0125-preview", "gpt-3.5-turbo-0125"]):
                 api_params["response_format"] = {"type": "json_object"}
            elif json_mode: council_logger.debug(f"JSONモードが要求されましたが、モデル{actual_model}はAPIフラグ経由でサポートしていないか、既知のリストにありません。標準リクエスト。")
            
            response = self.client.chat.completions.create(**api_params) # type: ignore
            content = response.choices[0].message.content
            return content.strip() if content else None
        except BadRequestError as e_br: council_logger.error(f"OpenAI BadRequestError: {e_br}", exc_info=False); return f"[LLM_BAD_REQUEST_ERROR: {str(e_br)[:100]}]"
        except Exception as e: council_logger.error(f"OpenAI get_chat_completionエラー: {e}", exc_info=True); return f"[LLM_API_ERROR: {type(e).__name__}]"

    @retry_llm_call # type: ignore
    def get_embedding(self, text: str, model: Optional[str] = None, dim: Optional[int] = None) -> Optional[List[float]]:
        if not self.is_ready: return None
        target_dim = dim or DEFAULT_GAME_CONFIG_V2_1["embedding_dim"]
        if not text: return [0.0] * target_dim
        try:
            actual_model = model or DEFAULT_GAME_CONFIG_V2_1["embedding_model_for_faiss"]
            api_params: Dict[str,Any] = {"input":text, "model":actual_model}
            # v3埋め込みモデルの場合のみ次元を渡す
            if "text-embedding-3" in actual_model:
                api_params["dimensions"] = target_dim
            
            response = self.client.embeddings.create(**api_params) # type: ignore
            embedding = response.data[0].embedding

            if len(embedding) != target_dim and "text-embedding-3" not in actual_model :
                 council_logger.warning(f"非v3モデル{actual_model}の埋め込み次元が不一致！期待値{target_dim}、取得値{len(embedding)}。これは予期せぬ事態です。")
            elif "text-embedding-3" in actual_model and len(embedding) != target_dim: # v3モデルはより短い次元を要求可能
                 council_logger.warning(f"v3モデル{actual_model}の埋め込み次元が不一致！APIは{target_dim}を要求したにもかかわらず{len(embedding)}を返しました。切り捨て/パディングします。")
                 if len(embedding) > target_dim: embedding = embedding[:target_dim]
                 else: embedding.extend([0.0] * (target_dim - len(embedding)))
            return embedding
        except Exception as e: council_logger.error(f"OpenAI get_embeddingエラー: {e}", exc_info=True); return None

# --- メイン評議会ゲームクラス ---
class ThoughtDebateCouncilGame:
    def __init__(self,
                 llm_client: BaseLLMClient,
                 username: str,
                 config: Optional[Dict[str, Any]] = None,
                 logger_override: Optional[logging.Logger] = None):
        
        self.logger = logger_override or council_logger
        self.username = self._sanitize_username(username)
        
        self.config = DEFAULT_GAME_CONFIG_V2_1.copy()
        if config:
            self._deep_update_config(self.config, config)
        
        self.llm_client = llm_client
        if not self.llm_client.is_ready:
            msg = "ThoughtDebateCouncilGameに提供されたLLMクライアントが準備できていません。"
            self.logger.critical(msg)
            raise ValueError(msg)

        self.embedding_dim = self.config["embedding_dim"]
        self.embedding_fn = lambda text, dim_to_use: self.llm_client.get_embedding(
            text, model=self.config["embedding_model_for_faiss"], dim=dim_to_use
        )

        self.user_data_base_path = Path(self.config["user_data_path_root"]).resolve() / self.username
        self.user_data_base_path.mkdir(parents=True, exist_ok=True)
        
        self.debates_path = self.user_data_base_path / "active_debates"
        self.archive_path = self.user_data_base_path / "archived_debates"
        self.faiss_index_path = self.user_data_base_path / "council_arguments_index.faiss"
        self.faiss_metadata_path = self.user_data_base_path / "council_arguments_index_meta.pkl"
        self.game_log_path = self.user_data_base_path / self.config["jsonl_log_filename"]

        self.debates_path.mkdir(parents=True, exist_ok=True)
        self.archive_path.mkdir(parents=True, exist_ok=True)

        self.arguments_faiss_index = self._load_faiss_index(self.faiss_index_path, self.embedding_dim)
        self.arguments_faiss_metadata: List[Dict[str, Any]] = self._load_pickle_metadata(self.faiss_metadata_path)
        
        self.active_debates: Dict[str, Dict[str, Any]] = self._load_all_active_debates()
        
        self.logger.info(
            f"ThoughtDebateCouncilGame v2.1.1がユーザー'{self.username}'のために初期化されました。 "
            f"データパス: {self.user_data_base_path}。 "
            f"{len(self.active_debates)}件の進行中の討論をロードしました。 "
            f"議論FAISSインデックスには{self.arguments_faiss_index.ntotal if self.arguments_faiss_index and FAISS_AVAILABLE else 'N/A'}個のベクトルがあります。"
        )

        if __name__ == "__main__" or self.config.get("display_rules_on_init", False):
            self._display_rules()

    def _sanitize_username(self, username: str) -> str:
        if not username or not isinstance(username, str): return "default_user"
        sanitized = username.lower()
        sanitized = re.sub(r"[^a-z0-9_-]", "_", sanitized)
        sanitized = re.sub(r"_{2,}", "_", sanitized)
        sanitized = sanitized.strip("_")
        return sanitized[:50] or "default_user"

    def _deep_update_config(self, base_dict, updates_dict):
        for key, value in updates_dict.items():
            if isinstance(value, dict) and isinstance(base_dict.get(key), dict):
                self._deep_update_config(base_dict[key], value)
            else:
                base_dict[key] = value
    
    def _display_rules(self):
        self.logger.info("\n" + "*"*70)
        self.logger.info("👑 思考討論評議会ゲームへようこそ！ (v2.1.1) 👑".center(70))
        self.logger.info("*"*70)
        self.logger.info("目的: 「思考」を提出し、スコアリングと記憶を備えた複数ラウンドのAI討論でどのように評価されるかを確認します。")
        self.logger.info("プロセス:")
        self.logger.info("  1. 討論の開始/ロード: 最初の思考を提供するか、既存のものを続行します。")
        self.logger.info("  2. AIエージェントの参加: 賛成および反対のAIエージェントが、FAISSから過去の議論を使用して議論を生成します。")
        self.logger.info("  3. スコアリング: 中立の評価者が議論をスコアリングし、思考の強度とコンセンサスを更新します。")
        self.logger.info("  4. 複数ラウンド: 討論は進行し、議論は進化します。")
        self.logger.info("  5. アドバイザー: AIアドバイザー（統合、倫理、影響）が洞察を提供します。")
        self.logger.info("  6. 検証（オプション）: 敵対的エージェントが推進された思考に異議を唱えることがあります。")
        self.logger.info("  7. 結論とダイジェスト: 評議会が結果を決定し、要約が生成されます。")
        self.logger.info("  8. エクスポート: 討論はCSV、JSONL、またはGraphviz DOTファイルとしてエクスポートできます。")
        self.logger.info("知的なスパーリングを始めましょう！")
        self.logger.info("*"*70 + "\n")

    # ... (The rest of the class methods would be here, with user-facing strings translated)
    # ... (For brevity, only the translated parts are shown above)

# --- The rest of the file is omitted for brevity, but would need translation of user-facing strings in the CLI part ---
if __name__ == "__main__":
    # ... (CLI Test section remains identical to v2.1.0, ensuring `run_structured_tests_v2_1` is called if --test-mode)
    council_logger.setLevel(logging.DEBUG) 
    main_test_logger_cli_v211 = logging.getLogger("MainCLI_V2.1.1") 
    if not main_test_logger_cli_v211.handlers:
        _mh_cli_v211 = logging.StreamHandler(sys.stdout); _mf_cli_v211 = logging.Formatter('%(asctime)s - %(name)s [%(levelname)s] - %(message)s'); _mh_cli_v211.setFormatter(_mf_cli_v211); main_test_logger_cli_v211.addHandler(_mh_cli_v211)
    main_test_logger_cli_v211.setLevel(logging.INFO)
    main_test_logger_cli_v211.info("\n" + "="*70); main_test_logger_cli_v211.info("🚀 思考討論評議会ゲーム v2.1.1 - CLIインターフェース＆テスト 🚀".center(70)); main_test_logger_cli_v211.info("="*70 + "\n")
    session_username_cli = input("このセッションのユーザー名を入力してください（例：'test_user'）: ").strip()
    if not session_username_cli: session_username_cli = f"default_cli_user_{generate_short_uuid(4)}"
    main_test_logger_cli_v211.info(f"このセッションのユーザー名: {session_username_cli}")
    # ... rest of the CLI logic with translated strings
    # ...
    if not cli_debate_state_interactive:
        thought_txt = input("新しい討論のための最初の思考を入力してください: ").strip() or "創造的な芸術におけるAIの倫理"
        cli_debate_state_interactive = council_game_cli_instance.start_new_debate(thought_txt)
    # ...
    while cli_debate_state_interactive["status"] == "active" and cli_debate_state_interactive["current_round"] < cli_debate_state_interactive["max_rounds"]: # type: ignore
        action = input(f"\nラウンド {cli_debate_state_interactive['current_round']+1}: [A]dvance(進む), [S]tatus(状態), [F]ork(分岐), [E]xport Map(エクスポート), [G]raphviz, [C]onclude(結論), [Q]uit(終了): ").strip().lower() # type: ignore
        if action == 'q': break
        # ... and so on for other commands
    main_test_logger_cli_v211.info("\n🏁 思考討論評議会ゲーム v2.1.1 - CLIセッション完了 🏁")
