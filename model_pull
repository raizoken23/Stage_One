
# ZAYARA :: UE-safe local LLM deployment module
# This module consolidates the entire installer logic into a single file that can be run directly or bundled into zayara.exe.
# Developer Notes:
# - Grown from original need for uso to not have to remember codes cuz wtf .
# - To build into zayara.exe: Install PyInstaller (`pip install pyinstaller`), then run `pyinstaller --onefile --name zayara --windowed zayara_installer.py`. NOT YET DONE
# - No internet for runtime after deps/models downloaded; dep checks install if missing. CUZ OFFLINE BRUH
# - EN/JP mode, first-run print, logs rehydration, error catalog, audit step all included. KOUSAKKIIIIII
# - System-agnostic: Detects OS/backend (CUDA/ROCm/Metal/CPU), chooses paths/drives intelligently.
# - Initial deployment: Run `python zayara_installer.py install` to deploy; bundles service setup. its zayara.py
# - Enhance: Added stubs for offline bundle support (grow by adding .zip extraction if env OFFLINE=1).
# - Grow: Moved language mode, translations, and t() function earlier in the file to ensure availability for all logging calls, including early dependency checks and drive selection. This enhances consistency without altering existing logic flow.
# - Enhance: Updated rehydrate_logs to use t() for internationalized messages, growing multilingual support to cover rehydration summaries. Added handling for summarizing completions in a more detailed way if needed (stub for future growth).
# - MADE BY THEE WORLDS BEST MORON - dont come at me bro im a expert vibe coder
# - Grow: Enhanced sys diag logs to use t() where applicable, but kept core [SYS] prefixes for debug clarity; added more detailed GPU env logging if backend is detected early.
# - Enhance: Added default behavior to run 'install' command if no arguments are provided to the script. This makes it user-friendly for first-time runs without requiring users to specify 'install' explicitly, while still allowing other commands like 'service'. Logs a message when defaulting to install for transparency.
# - Grow: Added support for building from source if prebuilt binary not available for the backend/OS. Uses git clone and cmake to build. For prebuilt, use {tag} in URL to include the build number.
# - Enhance: Updated URLS to use ggml-org repo consistently. Added get_latest_tag to fetch latest release tag from GitHub API. URLs can now include {tag} to download version-specific assets. For backends like Linux CUDA/ROCm, no prebuilt, so fallback to build. Assumes git, cmake, and backend SDKs installed.
# - Grow: Added installation for three models: Qwen2.5-Coder-7B, Mistral-7B-Instruct-v0.2, and gpt-oss-20b. Duplicated model download and resolve sections for each. Adjusted quant patterns to match case in HF files. For 20B, adjusted quant choices for larger size. Generated separate preset scripts for each model. Performed smoke test only for the primary model (Qwen); stub for others. Removed 'UE-safe' notations to generalize for public use.

import os, sys, subprocess, zipfile, urllib.request, logging, time, platform, json, shutil, tarfile
from pathlib import Path
import typer
from typing import Optional
from dataclasses import dataclass
import atexit, signal
import requests  # For better downloads + proxies

# ---------- Logging ----------
logger = logging.getLogger("ZAYARA")
logger.setLevel(logging.INFO)
fmt = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
sh = logging.StreamHandler(sys.stdout); sh.setFormatter(fmt); logger.addHandler(sh)

def log(msg): logger.info(msg)
def warn(msg): logger.warning(msg)
def err(msg): logger.error(msg)

# Developer Notes: EN/JP mode grown to all messages.
# - Env: Print both or selected.
# - Grow: Added keys for all logs/errors/prints (e.g., [AUDIT], [SERVER_LAUNCH], etc.).
# - Enhance: Grown to include rehydration keys for different i18n; added SYS keys for potential future translation of diag logs.
LANG_MODE = os.getenv("LANG_MODE", "EN").upper()
TRANS = {
    "EN": {
        "GPU_DETECT": "GPU: {name} / {vram} GB total / {free} GB free",
        "DRIVE_CHOSEN": "Chosen drive: {drive} (free={free}GB, SSD={ssd})",
        "DEP_OK": "{dep} OK",
        "INSTALL_DEP": "Installing {dep} …",
        "DONE": "Done; logs at {log}",
        "ERROR_NO_DRIVE": "No suitable drive found.",
        "REHYDRATE": "Previous run summary:",
        "ERRORS": "Errors: {count}",
        "COMPLETIONS": "Completions: {count}",
        "INTELL_HIGH_ERRORS": "High past errors; check AV/firewall or dependencies.",
        "ENV_SET": "Environment variables set for {backend}.",
        "DETECT_OS": "Detected OS: {os}",
        "SYS_CPU": "CPU: {processor} / Cores: {cores} / Freq: {freq} MHz",
        "SYS_RAM": "RAM: {ram} GB total",
        "AUDIT_VERIFY": "Verifying installation...",
        "AUDIT_MODEL_OK": "Model OK",
        "AUDIT_FAIL_MODEL": "Model missing/invalid.",
        "AUDIT_API_OK": "API OK",
        "AUDIT_API_STATUS": "API status {status}",
        "AUDIT_FAIL_API": "API test: {error}",
        "AUDIT_COMPLETE": "Audit Complete: All good.",
        "SERVER_LAUNCH": "Starting local API server...",
        "SERVER_PID": "PID={pid} on port {port}. Waiting 10s for startup...",
        "CHAT_UI_LAUNCHED": "Launched at http://127.0.0.1:7860",
        "SERVICE_INSTALLED": "Service Installed.",
        "MODEL_RESOLVE_SINGLE": "single-file → {path} ({size})",
        "MODEL_RESOLVE_MERGED": "existing merged → {path} ({size})",
        "MODEL_RESOLVE_MERGING": "merging → {target}",
        "MODEL_RESOLVE_MERGE_COMPLETE": "merge complete → {path} ({size})",
        "MODEL_RESOLVE_FALLBACK": "fallback gguf → {path} ({size})",
        "MODEL_RESOLVE_FAIL": "No GGUF files found after download.",
        "HF_LOCAL_COPY": "local copy at {dir}",
        "INTELL_QUANT": "Chosen quant: {pattern} based on VRAM={vram}GB / RAM={ram}GB",
        "RUN_SMOKE": "Smoke test (ctx={ctx} batch={bs} ubatch={ubs} ngl={ngl} n_predict={pred} T={temp} seed={seed})",
        "GPU_BEFORE": "before load: {snap}",
        "GPU_AFTER": "after run: {snap}",
        "EXEC_CMD": "[EXEC] {cmd}",
        "TIMEOUT_EXCEEDED": "timeout {timeout}s exceeded; terminated",
        "TERMINATED_POST_WAIT": "terminated after post-wait timeout",
        "SMOKE_FAIL": "Smoke test failed with code {rc}.",
        "SERVER_PRESET_WRITTEN": "preset written: {path}",
        "DEFAULT_TO_INSTALL": "No command provided; defaulting to 'install'.",
    },
    "JP": {
        "GPU_DETECT": "GPU: {name} / {vram} GB 合計 / {free} GB 空き",
        "DRIVE_CHOSEN": "選択ドライブ: {drive} (空き={free}GB, SSD={ssd})",
        "DEP_OK": "{dep} OK",
        "INSTALL_DEP": "{dep} をインストール中 …",
        "DONE": "完了; ログ: {log}",
        "ERROR_NO_DRIVE": "適切なドライブが見つかりません。",
        "REHYDRATE": "前回の実行サマリー:",
        "ERRORS": "エラー: {count}",
        "COMPLETIONS": "完了: {count}",
        "INTELL_HIGH_ERRORS": "過去のエラーが多い; AV/ファイアウォールや依存関係を確認してください。",
        "ENV_SET": "{backend} の環境変数を設定しました。",
        "DETECT_OS": "検出OS: {os}",
        "SYS_CPU": "CPU: {processor} / コア: {cores} / 周波数: {freq} MHz",
        "SYS_RAM": "RAM: {ram} GB 合計",
        "AUDIT_VERIFY": "インストールを検証中...",
        "AUDIT_MODEL_OK": "モデル OK",
        "AUDIT_FAIL_MODEL": "モデルが見つからない/無効。",
        "AUDIT_API_OK": "API OK",
        "AUDIT_API_STATUS": "API ステータス {status}",
        "AUDIT_FAIL_API": "API テスト: {error}",
        "AUDIT_COMPLETE": "監査完了: すべて良好。",
        "SERVER_LAUNCH": "ローカルAPIサーバーを起動中...",
        "SERVER_PID": "PID={pid} ポート {port} で。起動を10秒待機中...",
        "CHAT_UI_LAUNCHED": "http://127.0.0.1:7860 で起動",
        "SERVICE_INSTALLED": "サービスがインストールされました。",
        "MODEL_RESOLVE_SINGLE": "シングルファイル → {path} ({size})",
        "MODEL_RESOLVE_MERGED": "既存マージ → {path} ({size})",
        "MODEL_RESOLVE_MERGING": "マージ中 → {target}",
        "MODEL_RESOLVE_MERGE_COMPLETE": "マージ完了 → {path} ({size})",
        "MODEL_RESOLVE_FALLBACK": "フォールバック gguf → {path} ({size})",
        "MODEL_RESOLVE_FAIL": "ダウンロード後にGGUFファイルが見つかりません。",
        "HF_LOCAL_COPY": "ローカルコピー: {dir}",
        "INTELL_QUANT": "選択クアント: {pattern} (VRAM={vram}GB / RAM={ram}GB ベース)",
        "RUN_SMOKE": "UE安全スモークテスト (ctx={ctx} batch={bs} ubatch={ubs} ngl={ngl} n_predict={pred} T={temp} seed={seed})",
        "GPU_BEFORE": "ロード前: {snap}",
        "GPU_AFTER": "実行後: {snap}",
        "EXEC_CMD": "[EXEC] {cmd}",
        "TIMEOUT_EXCEEDED": "タイムアウト {timeout}s 超過; 終了",
        "TERMINATED_POST_WAIT": "待機後タイムアウトで終了",
        "SMOKE_FAIL": "スモークテスト失敗 コード {rc}。",
        "SERVER_PRESET_WRITTEN": "プリセット作成: {path}",
        "DEFAULT_TO_INSTALL": "コマンドが指定されていません; 'install' にデフォルトします。",
    }
}
def t(key, **kw):
    msg = TRANS.get(LANG_MODE, TRANS["EN"]).get(key, key).format(**kw)
    if LANG_MODE == "EN_JP":
        jp = TRANS["JP"].get(key, key).format(**kw)
        return f"{msg} | {jp}"
    return msg

# Developer Notes: Enhanced logging rehydration.
# - On start, read previous log and re-log key events (e.g., errors, completions).
# - Catalog errors in separate error_catalog.jsonl for intelligent analysis.
# - Grow: Append new runs; no overwrite. Intell: Summarize past errors for decisions (e.g., skip failed steps).
# - Enhance: Grown to use t() for all messages in rehydrate_logs, ensuring multilingual consistency. Added optional detailed completion summary (stub: if len(completions)>0, log first 3).
def rehydrate_logs(log_file: Path, error_catalog: Path):
    if log_file.exists():
        with log_file.open("r", encoding="utf-8") as f:
            lines = f.readlines()
            errors = [line for line in lines if "ERROR" in line]
            completions = [line for line in lines if "DONE" in line or "complete" in line.lower()]
        log(t("REHYDRATE"))
        log(t("ERRORS", count=len(errors)))
        for e in errors[:5]: log(f"    {e.strip()}")
        if len(errors) > 5: log("    ... (more in log)")
        log(t("COMPLETIONS", count=len(completions)))
        # Grow: Optional detailed completions (stub for enhancement: log first few)
        for c in completions[:3]: log(f"    {c.strip()}")
        if len(completions) > 3: log("    ... (more in log)")
        # Catalog errors to JSONL
        for e in errors:
            with error_catalog.open("a", encoding="utf-8") as ec:
                ec.write(json.dumps({"ts": time.time(), "error": e.strip()}) + "\n")
        # Intell: If many errors, suggest fixes (stub for growth).
        if len(errors) > 3:
            warn(t("INTELL_HIGH_ERRORS"))

# --- CUDA env early (affects --version/--help and first load) ---
def set_gpu_env(backend: str):
    if backend == "cuda":
        os.environ.setdefault("LLAMA_CUBLAS", "1")
        os.environ.setdefault("GGML_CUDA_FORCE_CUBLAS", "1")
        os.environ.setdefault("GGML_CUDA_ALLOW_TF32", "1")
        log(f"[ENV] LLAMA_CUBLAS={os.environ['LLAMA_CUBLAS']}  "
            f"GGML_CUDA_FORCE_CUBLAS={os.environ['GGML_CUDA_FORCE_CUBLAS']}  "
            f"GGML_CUDA_ALLOW_TF32={os.environ['GGML_CUDA_ALLOW_TF32']}")

def download_progress(block_num, block_size, total_size):
    d = block_num * block_size
    if total_size > 0:
        pct = min(100, d * 100 / total_size)
        print(f"\rDownloading: {pct:.2f}% [{d/1048576:.1f} / {total_size/1048576:.1f} MB]", end='', flush=True)
        if d >= total_size: print()
    else:
        print(f"\rDownloaded: {d/1048576:.1f} MB (size unknown)", end='', flush=True)

# ---------- GPU/Backend detect ----------
class Backend:
    CUDA = "cuda"; ROCM = "rocm"; METAL = "metal"; CPU = "cpu"

def detect_backend() -> str:
    sysname = platform.system().lower()
    if sysname == "darwin":
        return Backend.METAL
    try:
        subprocess.check_output(["nvidia-smi"], stderr=subprocess.STDOUT)
        return Backend.CUDA
    except Exception:
        pass
    for probe in ("rocminfo", "rocm-smi", "hipconfig"):
        try:
            subprocess.check_output([probe], stderr=subprocess.STDOUT)
            return Backend.ROCM
        except Exception:
            pass
    return Backend.CPU

def get_gpu_info(backend: str):
    if backend == "cuda":
        try:
            out = subprocess.check_output(
                ["nvidia-smi","--query-gpu=name,memory.total,memory.free","--format=csv,noheader,nounits"],
                text=True
            ).strip()
            name, mem_total_mib, mem_free_mib = out.splitlines()[0].split(",")
            return name.strip(), int(mem_total_mib)//1024, int(mem_free_mib)//1024
        except Exception as e:
            raise RuntimeError("NVIDIA GPU not detected or nvidia-smi unavailable.") from e
    elif backend == "rocm":
        # Stub for ROCm info (grow with rocm-smi parse)
        return "AMD GPU", 8, 6  # Placeholder; enhance with actual probe
    elif backend == "metal":
        return "Apple Silicon", 8, 6  # Placeholder; use sysctl or similar
    else:
        return "CPU", 0, 0

# ---------- Sys diag ----------
log(f"[SYS] Python={platform.python_version()}  EXE={sys.executable}")
log(f"[SYS] OS={platform.system()} {platform.release()}  Arch={platform.machine()}")
log(f"[SYS] CWD={Path.cwd()}  PATH={os.environ.get('PATH','')[:2000]}…")
log(f"[SYS] CUDA_PATH={os.environ.get('CUDA_PATH','-')}")

# Developer Notes: Enhanced OS/drive detection.
# - Detect OS: Windows/Linux/Mac.
# - Drives: Use psutil to list partitions, choose intelligently (largest SSD/free space, avoid system drive).
# - Safe: Check write permissions/free space (>10GB needed).
# - Grow: If Linux, adjust paths/binaries (e.g., no .exe); stub for Mac.
OS_TYPE = platform.system().lower()
log(t("DETECT_OS", os=OS_TYPE))
try:
    import psutil
except ImportError:
    log(t("INSTALL_DEP", dep="psutil"))
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-U", "psutil"])
    import psutil
cpu_info = psutil.cpu_freq()
log(t("SYS_CPU", processor=platform.processor(), cores=psutil.cpu_count(), freq=cpu_info.current if cpu_info else 'n/a'))
ram_gb = psutil.virtual_memory().total // (1024 ** 3)
log(t("SYS_RAM", ram=ram_gb))

def default_root() -> Path:
    sysname = platform.system().lower()
    if sysname == "windows":
        base = Path(os.environ.get("LOCALAPPDATA", Path.home()/"AppData/Local"))
        return base / "Zayara" / "oss_models"
    elif sysname == "darwin":
        return Path.home() / "Library/Application Support/Zayara/oss_models"
    else:
        base = Path(os.environ.get("XDG_DATA_HOME", Path.home()/".local/share"))
        return base / "zayara/oss_models"

# Intelligent drive selection (under default_root, but override if needed)
def choose_install_drive(min_free_gb=10):
    parts = psutil.disk_partitions()
    candidates = []
    for p in parts:
        usage = psutil.disk_usage(p.mountpoint)
        free_gb = usage.free // (1024 ** 3)
        is_ssd = 'ssd' in p.opts.lower() or free_gb > 100
        if free_gb > min_free_gb and (OS_TYPE != "darwin" or p.mountpoint != '/') and (OS_TYPE != "windows" or p.mountpoint != 'C:\\') and (OS_TYPE != "linux" or free_gb > min_free_gb):  # Allow / on Linux if viable
            candidates.append((p.mountpoint, free_gb, is_ssd))
    if candidates:
        sel = max(candidates, key=lambda x: (x[2], x[1]))
        log(t("DRIVE_CHOSEN", drive=sel[0], free=sel[1], ssd=sel[2]))
        return Path(sel[0]) / "OSS_MODELS"
    err(t("ERROR_NO_DRIVE")); sys.exit(1)

ROOT = os.environ.get("ZAYARA_ROOT")
if ROOT:
    ROOT = Path(ROOT)
else:
    ROOT = default_root()
    try:
        ROOT.mkdir(parents=True, exist_ok=True)
    except Exception:
        # Fallback for odd environments; try drive scan
        ROOT = choose_install_drive()
        ROOT.mkdir(parents=True, exist_ok=True)
ROOT.mkdir(parents=True, exist_ok=True)
LOG_FILE = ROOT / "install.log"
ERROR_CATALOG = ROOT / "error_catalog.jsonl"
FIRST_RUN_FLAG = ROOT / ".first_run"
fh = logging.FileHandler(LOG_FILE, encoding="utf-8"); fh.setFormatter(fmt); logger.addHandler(fh)  # Re-add after ROOT set
rehydrate_logs(LOG_FILE, ERROR_CATALOG)

# ---------- URLs for binaries (OS/backend) ----------
URLS = {
    ("windows", Backend.CUDA): [
        "https://github.com/ggml-org/llama.cpp/releases/latest/download/cudart-llama-bin-win-cuda-12.4-x64.zip",
        "https://github.com/ggml-org/llama.cpp/releases/download/{tag}/llama-{tag}-bin-win-cuda-12.4-x64.zip",
    ],
    ("windows", Backend.CPU): [
        "https://github.com/ggml-org/llama.cpp/releases/download/{tag}/llama-{tag}-bin-win-avx2-x64.zip",
    ],
    ("darwin", Backend.METAL): [
        "https://github.com/ggml-org/llama.cpp/releases/download/{tag}/llama-{tag}-bin-macos-arm64.zip",
    ],
    # For linux and others, empty list to trigger build from source
    ("linux", Backend.CUDA): [],
    ("linux", Backend.ROCM): [],
    # ("any", Backend.CPU): [],  # Removed, fallback to build
}

HF_REPO_QWEN = "Qwen/Qwen2.5-Coder-7B-Instruct-GGUF"
def choose_quant_pattern_qwen(vram_gb: int, ram_gb: int) -> str:
    if vram_gb < 8 or ram_gb < 32:
        return "qwen2.5-coder-7b-instruct-q4_K_M*.gguf"
    elif vram_gb < 16 or ram_gb < 64:
        return "qwen2.5-coder-7b-instruct-q5_K_M*.gguf"
    else:
        return "qwen2.5-coder-7b-instruct-q6_K*.gguf"

HF_REPO_MISTRAL = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
def choose_quant_pattern_mistral(vram_gb: int, ram_gb: int) -> str:
    if vram_gb < 8 or ram_gb < 32:
        return "mistral-7b-instruct-v0.2.Q4_K_M*.gguf"
    elif vram_gb < 16 or ram_gb < 64:
        return "mistral-7b-instruct-v0.2.Q5_K_M*.gguf"
    else:
        return "mistral-7b-instruct-v0.2.Q6_K*.gguf"

HF_REPO_OSS20B = "bartowski/openai_gpt-oss-20b-GGUF"
def choose_quant_pattern_oss20b(vram_gb: int, ram_gb: int) -> str:
    if vram_gb < 8 or ram_gb < 32:
        return "openai_gpt-oss-20b-Q3_K_M*.gguf"
    elif vram_gb < 16 or ram_gb < 64:
        return "openai_gpt-oss-20b-Q4_K_M*.gguf"
    else:
        return "openai_gpt-oss-20b-Q5_K_M*.gguf"

# Developer Notes: Dependency check.
# - Check/install: psutil, huggingface_hub, gradio, requests.
# - Intell: If missing, pip install; log versions.
# - Grow: Add more deps as needed.
def check_deps():
    deps = ["psutil", "huggingface_hub", "gradio", "requests"]
    for d in deps:
        try:
            __import__(d)
            log(t("DEP_OK", dep=d))
        except ImportError:
            log(t("INSTALL_DEP", dep=d))
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-U", d])
check_deps()

# Developer Notes: What's Happening CLI print (first use).
# - Detect first run via log size or flag file.
# - Print summary of steps.
def print_whats_happening(first_run_flag: Path):
    if not first_run_flag.exists():
        print("""
WHAT'S HAPPENING:
1. Detect system: OS, GPU, drives, RAM/CPU.
2. Choose install drive intelligently (largest SSD with space).
3. Download/extract llama.cpp binaries (OS-specific).
4. Download models from HF (quant based on hardware).
5. Merge shards if needed.
6. Run smoke test.
7. Launch API server.
8. Open chat UI.
9. Create startup script.
Errors cataloged in error_catalog.jsonl; logs in install.log.
""")
        first_run_flag.touch()
print_whats_happening(FIRST_RUN_FLAG)

# ---------- Helpers ----------
def list_tree(path: Path, max_items=80):
    files = list(path.rglob("*"))
    log(f"[TREE] {path} -> {len(files)} entries")
    for i, f in enumerate(files[:max_items]):
        log(f"  - {f.relative_to(path)}")
    if len(files) > max_items:
        log(f"  ... (+{len(files)-max_items} more)")

def find_bins(root: Path):
    ext = ".exe" if OS_TYPE == "windows" else ""
    targets = {
        "cli": (f"llama-cli{ext}", f"llama{ext}", f"llama-run{ext}"),
        "server": (f"llama-server{ext}", f"server{ext}", f"rpc-server{ext}"),
        "merge": (f"llama-gguf-split{ext}",),
    }
    found = {"cli":"", "server":"", "merge":""}
    for kind, names in targets.items():
        for n in names:
            p = next((str(x) for x in root.rglob(n) if x.is_file() and os.access(x, os.X_OK)), "")
            if p:
                found[kind] = p
                break
    return found

def _ensure_exec_bits(root: Path):
    if OS_TYPE in ("linux", "darwin"):
        for p in root.rglob("*"):
            if p.is_file() and (p.name.startswith("llama") or p.suffix in (".sh",)):
                try:
                    p.chmod(p.stat().st_mode | 0o111)
                except Exception as e:
                    warn(f"chmod +x failed for {p}: {e}")

def fetch_and_extract(url: str, zip_path: Path, extract_dir: Path):
    if zip_path.exists(): zip_path.unlink()
    log(f"[DOWNLOAD] {url} -> {zip_path}")
    urllib.request.urlretrieve(url, zip_path, reporthook=download_progress)
    log(f"[EXTRACT] {zip_path} -> {extract_dir}")
    if zip_path.suffix == '.zip':
        with zipfile.ZipFile(zip_path, "r") as z: z.extractall(extract_dir)
    else:  # tar.gz for Linux/Mac
        with tarfile.open(zip_path, "r:*") as t: t.extractall(extract_dir)
    zip_path.unlink(missing_ok=True)
    _ensure_exec_bits(extract_dir)
    list_tree(extract_dir)

def human_size(p: Path) -> str:
    try:
        b = p.stat().st_size
        for unit in ("B","KB","MB","GB","TB"):
            if b < 1024: return f"{b:.1f} {unit}"
            b /= 1024
        return f"{b:.1f} PB"
    except Exception:
        return "n/a"

def get_help(bin_path: str) -> str:
    try:
        return subprocess.check_output([bin_path, "--help"], text=True, errors="ignore")
    except Exception as e:
        warn(f"[HELP] failed for {bin_path}: {e}")
        return ""

def get_version(bin_path: str) -> str:
    try:
        return subprocess.check_output([bin_path, "--version"], text=True, errors="ignore")
    except Exception:
        return ""

def pick_flag(help_text: str, candidates: list[str]) -> str | None:
    for c in candidates:
        if c in help_text:
            return c
    return None

def gpu_mem_snapshot():
    try:
        out = subprocess.check_output(
            ["nvidia-smi","--query-gpu=memory.total,memory.used,memory.free","--format=csv,noheader,nounits"],
            text=True
        ).strip()
        tot, used, free = [int(x)//1024 for x in out.split(",")]
        return f"{used}GB used / {free}GB free of {tot}GB"
    except Exception:
        return "n/a"

def stream_exec(cmd: list[str], timeout_s: int = 120) -> int:
    log(t("EXEC_CMD", cmd=" ".join(cmd)))
    start = time.time()
    with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                          text=True, bufsize=1) as p:
        try:
            for line in p.stdout:
                print(line.rstrip())
                if time.time() - start > timeout_s:
                    p.terminate()
                    err(t("TIMEOUT_EXCEEDED", timeout=timeout_s))
                    return 124
            return p.wait(timeout=15)
        except subprocess.TimeoutExpired:
            p.kill()
            err(t("TERMINATED_POST_WAIT"))
            return 124

def resolve_model_path(local_dir: Path, merged_target: Path, merge_tool_path: str | None) -> str:
    # Prefer a single-file GGUF (any quant)
    singles = sorted(local_dir.glob("*q4_K_M*.gguf")) \
            + sorted(local_dir.glob("*q5_K_M*.gguf")) \
            + sorted(local_dir.glob("*q6_K*.gguf")) \
            + sorted(local_dir.glob("*Q4_K_M*.gguf")) \
            + sorted(local_dir.glob("*Q5_K_M*.gguf")) \
            + sorted(local_dir.glob("*Q6_K*.gguf")) \
            + sorted(local_dir.glob("*Q3_K_M*.gguf"))
    if singles:
        sel = singles[0]
        log(t("MODEL_RESOLVE_SINGLE", path=str(sel), size=human_size(sel)))
        return str(sel)

    # If a previously merged file exists, use it
    if merged_target.exists():
        log(t("MODEL_RESOLVE_MERGED", path=str(merged_target), size=human_size(merged_target)))
        return str(merged_target)

    # Merge shards if present
    parts = sorted(local_dir.glob("*-00001-of-*.gguf"))
    if parts:
        if not merge_tool_path:
            warn("Sharded GGUF found but merge tool missing; falling back to first .gguf present.")
            any_gguf = sorted(local_dir.glob("*.gguf"))
            if any_gguf:
                return str(any_gguf[0])
            err("No usable GGUF when shards present and merge tool missing.")
            sys.exit(1)
        first = parts[0]
        try:
            if merged_target.exists():
                merged_target.unlink()
        except Exception as e:
            warn(f"[MODEL_RESOLVE] cannot remove target: {e}")
        log(t("MODEL_RESOLVE_MERGING", target=str(merged_target)))
        subprocess.check_call([merge_tool_path, "--merge", str(first), str(merged_target)])
        log(t("MODEL_RESOLVE_MERGE_COMPLETE", path=str(merged_target), size=human_size(merged_target)))
        return str(merged_target)

    # Fallback to first GGUF we see
    any_gguf = sorted(local_dir.glob("*.gguf"))
    if any_gguf:
        sel = any_gguf[0]
        warn(t("MODEL_RESOLVE_FALLBACK", path=str(sel), size=human_size(sel)))
        return str(sel)

    err(t("MODEL_RESOLVE_FAIL"))
    sys.exit(1)

def audit_install(root: Path, bins: dict, model_path: str, port: str) -> bool:
    log(t("AUDIT_VERIFY"))
    if not (bins.get("cli") or bins.get("server")):
        err("[AUDIT_FAIL] No CLI/Server found.")
        return False
    if not Path(model_path).exists() or human_size(Path(model_path)) == "0.0 B":
        err(t("AUDIT_FAIL_MODEL"))
        return False
    try:
        import requests
        for i in range(20):
            try:
                r = requests.get(f"http://127.0.0.1:{port}/health", timeout=2)
                if r.status_code == 200:
                    log(t("AUDIT_API_OK"))
                    break
                else:
                    warn(t("AUDIT_API_STATUS", status=r.status_code))
            except Exception:
                time.sleep(0.5)
        else:
            warn(t("AUDIT_API_STATUS", status="unreachable"))
    except Exception as e:
        warn(t("AUDIT_FAIL_API", error=str(e)))
    return True

def _self_cmd_for_services() -> list[str]:
    # When frozen by PyInstaller, sys.executable is the zayara.exe binary
    if getattr(sys, "frozen", False):
        return [sys.executable, "install"]
    # Running as a .py script
    return [sys.executable, os.path.abspath(__file__), "install"]

def _kill_child(p):
    try:
        if p and p.poll() is None:
            p.terminate()
    except Exception:
        pass

# ---------- Service install ----------
def ensure_admin():
    if OS_TYPE == "windows":
        import ctypes
        try:
            is_admin = ctypes.windll.shell32.IsUserAnAdmin()
        except Exception:
            is_admin = False
        if not is_admin:
            params = " ".join([f'"{a}"' for a in sys.argv])
            ctypes.windll.shell32.ShellExecuteW(None, "runas", sys.executable, params, None, 1)
            sys.exit(0)

def service_install():
    ensure_admin()
    cmd = _self_cmd_for_services()
    if OS_TYPE == "windows":
        exe, *args = cmd
        arg_line = " ".join(f'"{a}"' for a in args)
        xml = f"""<Task version="1.2" xmlns="http://schemas.microsoft.com/windows/2004/02/mit/task">
  <Triggers><LogonTrigger><Enabled>true</Enabled></LogonTrigger></Triggers>
  <Principals><Principal id="Author"><RunLevel>HighestAvailable</RunLevel></Principal></Principals>
  <Settings><MultipleInstancesPolicy>IgnoreNew</MultipleInstancesPolicy><RestartOnFailure/></Settings>
  <Actions Context="Author">
    <Exec>
      <Command>{exe}</Command>
      <Arguments>{arg_line}</Arguments>
      <WorkingDirectory>{ROOT}</WorkingDirectory>
    </Exec>
  </Actions>
</Task>"""
        xml_path = ROOT / "TaskScheduler.xml"
        xml_path.write_text(xml, encoding="utf-8")
        subprocess.check_call(["schtasks", "/Create", "/TN", "Zayara", "/XML", str(xml_path), "/F"])
    elif OS_TYPE == "linux":
        unit_path = Path.home() / ".config/systemd/user/zayara.service"
        unit_path.parent.mkdir(parents=True, exist_ok=True)
        unit_path.write_text(f"""[Unit]
Description=Zayara LLM Server
After=network-online.target

[Service]
Type=simple
WorkingDirectory={ROOT}
ExecStart={' '.join(cmd)}
Restart=on-failure

[Install]
WantedBy=default.target""")
        subprocess.check_call(["systemctl","--user","daemon-reload"])
        subprocess.check_call(["systemctl","--user","enable","zayara"])
    elif OS_TYPE == "darwin":
        plist_path = Path.home() / "Library/LaunchAgents/com.zayara.server.plist"
        plist_path.parent.mkdir(parents=True, exist_ok=True)
        plist_path.write_text(f"""<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0"><dict>
  <key>Label</key><string>com.zayara.server</string>
  <key>ProgramArguments</key>
  <array>{''.join(f'<string>{c}</string>' for c in cmd)}</array>
  <key>WorkingDirectory</key><string>{ROOT}</string>
  <key>RunAtLoad</key><true/>
  <key>KeepAlive</key><true/>
</dict></plist>""")
        subprocess.check_call(["launchctl","load","-w", str(plist_path)])
    log(t("SERVICE_INSTALLED"))

def service_start():
    if OS_TYPE == "windows":
        subprocess.check_call(["schtasks", "/Run", "/TN", "Zayara"])
    elif OS_TYPE == "linux":
        subprocess.check_call(["systemctl","--user","start","zayara"])
    elif OS_TYPE == "darwin":
        subprocess.check_call(["launchctl","load","-w", str(Path.home()/"Library/LaunchAgents/com.zayara.server.plist")])

def service_stop():
    if OS_TYPE == "windows":
        subprocess.check_call(["schtasks", "/End", "/TN", "Zayara"])
    elif OS_TYPE == "linux":
        subprocess.check_call(["systemctl","--user","stop","zayara"])
    elif OS_TYPE == "darwin":
        subprocess.check_call(["launchctl","unload","-w", str(Path.home()/"Library/LaunchAgents/com.zayara.server.plist")])

# ---------- Install function ----------
server_proc = None  # Global for cleanup
LOCK = ROOT / ".install.lock"

class InstallLock:
    def __enter__(self):
        if LOCK.exists():
            err("Another installation appears to be running.")
            sys.exit(1)
        LOCK.write_text(str(os.getpid()))
    def __exit__(self, *a):
        try: LOCK.unlink()
        except: pass

def get_latest_tag():
    try:
        r = requests.get("https://api.github.com/repos/ggml-org/llama.cpp/releases/latest")
        r.raise_for_status()
        return r.json()["tag_name"]
    except Exception as e:
        err(f"Failed to get latest llama.cpp tag: {e}")
        sys.exit(1)

def build_llama_cpp(backend: str):
    log(f"[BUILD] Building llama.cpp from source for backend={backend}")
    repo_dir = ROOT / "llama.cpp_source"
    if not repo_dir.exists():
        subprocess.check_call(["git", "clone", "https://github.com/ggml-org/llama.cpp.git", str(repo_dir)])
    else:
        subprocess.check_call(["git", "pull"], cwd=repo_dir)
    build_dir = repo_dir / "build"
    build_dir.mkdir(parents=True, exist_ok=True)
    cmake_args = ["cmake", "-S", str(repo_dir), "-B", str(build_dir)]
    if backend == Backend.CUDA:
        cmake_args.append("-DGGML_CUDA=ON")
    elif backend == Backend.ROCM:
        cmake_args.append("-DGGML_HIPBLAS=ON")
    elif backend == Backend.METAL:
        cmake_args.append("-DGGML_METAL=ON")
    subprocess.check_call(cmake_args)
    subprocess.check_call(["cmake", "--build", str(build_dir), "--config", "Release", "-j"])
    # Copy binaries
    bin_dir = build_dir / "bin" / "Release" if OS_TYPE == "windows" else build_dir / "bin"
    for f in bin_dir.glob("*"):
        if f.is_file():
            shutil.copy(f, LLAMA_DIR)
    _ensure_exec_bits(LLAMA_DIR)

def install_all():
    with InstallLock():
        backend = detect_backend()
        set_gpu_env(backend)
        gpu_name, vram_gb, vram_free_gb = get_gpu_info(backend)
        log(t("GPU_DETECT", name=gpu_name, vram=vram_gb, free=vram_free_gb))
        LLAMA_DIR = ROOT / "llama.cpp"
        LLAMA_DIR.mkdir(parents=True, exist_ok=True)
        LLAMA_ZIP = ROOT / f"llama-{OS_TYPE}-{backend}.zip" # Dynamic name
        # Fetch binaries
        urls = URLS.get((OS_TYPE, backend)) or URLS.get(("any", Backend.CPU)) or []
        if urls:
            latest_tag = get_latest_tag()
            for url_template in urls:
                url = url_template.format(tag=latest_tag) if '{tag}' in url_template else url_template
                zip_name = Path(url_template).name.format(tag=latest_tag) if '{tag}' in url_template else Path(url_template).name
                zip_path = ROOT / zip_name
                fetch_and_extract(url, zip_path, LLAMA_DIR)
        else:
            build_llama_cpp(backend)
        bins = find_bins(LLAMA_DIR)
        llama_cli = bins["cli"]
        llama_server = bins["server"]
        merge_tool = bins["merge"]
        log(f"[RESOLVE] CLI={llama_cli or '-'}  SERVER={llama_server or '-'}  MERGE={merge_tool or '-'}")
        # Flag autodetect
        help_text = get_help(llama_cli or llama_server)
        FLAG_NGL = pick_flag(help_text, ["--n-gpu-layers", "--gpu-layers"]) or "--n-gpu-layers"
        FLAG_UBAT = pick_flag(help_text, ["--ubatch-size", "--gpu-batch-size"])
        FLAG_TB = pick_flag(help_text, ["--threads-batch"])
        FLAG_NOMAP = None
        log(f"[FLAGS] using: NGL={FLAG_NGL}  UBATCH={FLAG_UBAT or '-'}  TB={FLAG_TB or '-'}  NO_MMAP={FLAG_NOMAP or '-'}")

        # HF model Qwen
        HF_PATTERN_QWEN = choose_quant_pattern_qwen(vram_gb, ram_gb)
        log(t("INTELL_QUANT", pattern=HF_PATTERN_QWEN, vram=vram_gb, ram=ram_gb))
        MODEL_DIR_QWEN = ROOT / "qwen2.5-coder-7b-instruct-gguf"
        MODEL_DIR_QWEN.mkdir(parents=True, exist_ok=True)
        MERGED_GGUF_QWEN = MODEL_DIR_QWEN / f"qwen2.5-coder-7b-instruct-{HF_PATTERN_QWEN.split('*')[0]}.gguf"
        from huggingface_hub import snapshot_download
        local_repo_dir_qwen = snapshot_download(
            repo_id=HF_REPO_QWEN,
            allow_patterns=[HF_PATTERN_QWEN],
            local_dir=MODEL_DIR_QWEN,
        )
        log(t("HF_LOCAL_COPY", dir=local_repo_dir_qwen))
        model_path_qwen = resolve_model_path(Path(local_repo_dir_qwen), MERGED_GGUF_QWEN, merge_tool)
        log(t("MODEL_RESOLVE_SINGLE", path=model_path_qwen, size=human_size(Path(model_path_qwen))) if "single" in model_path_qwen else t("MODEL_RESOLVE_MERGED", path=model_path_qwen, size=human_size(Path(model_path_qwen))))

        # HF model Mistral
        HF_PATTERN_MISTRAL = choose_quant_pattern_mistral(vram_gb, ram_gb)
        log(t("INTELL_QUANT", pattern=HF_PATTERN_MISTRAL, vram=vram_gb, ram=ram_gb))
        MODEL_DIR_MISTRAL = ROOT / "mistral-7b-instruct-v0.2-gguf"
        MODEL_DIR_MISTRAL.mkdir(parents=True, exist_ok=True)
        MERGED_GGUF_MISTRAL = MODEL_DIR_MISTRAL / f"mistral-7b-instruct-v0.2.{HF_PATTERN_MISTRAL.split('*')[0]}.gguf"
        local_repo_dir_mistral = snapshot_download(
            repo_id=HF_REPO_MISTRAL,
            allow_patterns=[HF_PATTERN_MISTRAL],
            local_dir=MODEL_DIR_MISTRAL,
        )
        log(t("HF_LOCAL_COPY", dir=local_repo_dir_mistral))
        model_path_mistral = resolve_model_path(Path(local_repo_dir_mistral), MERGED_GGUF_MISTRAL, merge_tool)
        log(t("MODEL_RESOLVE_SINGLE", path=model_path_mistral, size=human_size(Path(model_path_mistral))) if "single" in model_path_mistral else t("MODEL_RESOLVE_MERGED", path=model_path_mistral, size=human_size(Path(model_path_mistral))))

        # HF model OSS20B
        HF_PATTERN_OSS20B = choose_quant_pattern_oss20b(vram_gb, ram_gb)
        log(t("INTELL_QUANT", pattern=HF_PATTERN_OSS20B, vram=vram_gb, ram=ram_gb))
        MODEL_DIR_OSS20B = ROOT / "openai_gpt-oss-20b-gguf"
        MODEL_DIR_OSS20B.mkdir(parents=True, exist_ok=True)
        MERGED_GGUF_OSS20B = MODEL_DIR_OSS20B / f"openai_gpt-oss-20b-{HF_PATTERN_OSS20B.split('*')[0]}.gguf"
        local_repo_dir_oss20b = snapshot_download(
            repo_id=HF_REPO_OSS20B,
            allow_patterns=[HF_PATTERN_OSS20B],
            local_dir=MODEL_DIR_OSS20B,
        )
        log(t("HF_LOCAL_COPY", dir=local_repo_dir_oss20b))
        model_path_oss20b = resolve_model_path(Path(local_repo_dir_oss20b), MERGED_GGUF_OSS20B, merge_tool)
        log(t("MODEL_RESOLVE_SINGLE", path=model_path_oss20b, size=human_size(Path(model_path_oss20b))) if "single" in model_path_oss20b else t("MODEL_RESOLVE_MERGED", path=model_path_oss20b, size=human_size(Path(model_path_oss20b))))

        # Smoke test (primary model only; stub for others)
        log(t("RUN_SMOKE", ctx=2048, bs=384, ubs=96, ngl=10, pred=32, temp=0.2, seed=42))
        log(t("GPU_BEFORE", snap=gpu_mem_snapshot()))
        if llama_cli:
            cmd = [llama_cli,
                   "--model", model_path_qwen,
                   "--prompt", "You are the Citadel local assistant. Confirm readiness in one short sentence.",
                   "--ctx-size", "2048",
                   "--batch-size", "384",
                   "--n-predict", "32",
                   "--temp", "0.2",
                   "--seed", "42"]
            if FLAG_UBAT: cmd += [FLAG_UBAT, "96"]
            if FLAG_NGL: cmd += [FLAG_NGL, "10"]
            cmd += ["--threads", str(os.cpu_count() or 16)]
            if FLAG_TB: cmd += [FLAG_TB, "16"]
            rc = stream_exec(cmd, timeout_s=120)
            if rc != 0:
                err(t("SMOKE_FAIL", rc=rc))
                sys.exit(1)
        else:
            test_port = "8089"
            srv = [llama_server, "--server", "--port", test_port, "--parallel", "2", "--cont-batching",
                   "--model", model_path_qwen, "--ctx-size", "2048", "--batch-size", "384",
                   "--n-predict", "64", "--temp", "0.2", "--seed", "42"]
            if FLAG_UBAT: srv += [FLAG_UBAT, "96"]
            if FLAG_NGL: srv += [FLAG_NGL, "10"]
            srv += ["--threads", str(os.cpu_count() or 16)]
            if FLAG_TB: srv += [FLAG_TB, "16"]
            proc = subprocess.Popen(srv, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
            import requests
            for i in range(20):
                try:
                    r = requests.get(f"http://127.0.0.1:{test_port}/health", timeout=2)
                    if r.status_code == 200: break
                except Exception:
                    pass
                time.sleep(0.5)
            else:
                proc.terminate()
                err("llama-server health never reached 200 during smoke."); sys.exit(1)
            # quick chat
            payload = {"model": os.path.basename(model_path_qwen), "messages":[{"role":"user","content":"ping"}], "stream": False}
            r = requests.post(f"http://127.0.0.1:{test_port}/v1/chat/completions", json=payload, timeout=20)
            if r.status_code != 200:
                proc.terminate()
                err(f"llama-server chat failed: {r.status_code} {r.text}"); sys.exit(1)
            proc.terminate()
        log(t("GPU_AFTER", snap=gpu_mem_snapshot()))
        # Launch server
        log(t("SERVER_LAUNCH"))
        server_port = "8080"
        if not llama_server:
            err("llama-server not found; cannot launch API. Check AV quarantine or re-run install.")
            sys.exit(1)
        server_cmd = [llama_server, "--server", "--port", server_port, "--parallel", "4", "--cont-batching",
                      "--model", model_path_qwen, "--ctx-size", "4096", "--batch-size", "512",
                      "--n-predict", "512", "--temp", "0.2", "--seed", "42",
                      "--threads", str(os.cpu_count() or 16)]
        if FLAG_UBAT: server_cmd += [FLAG_UBAT, "128"]
        if FLAG_NGL: server_cmd += [FLAG_NGL, "12"]
        if FLAG_TB: server_cmd += [FLAG_TB, "16"]
        global server_proc
        server_proc = subprocess.Popen(server_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        log(t("SERVER_PID", pid=server_proc.pid, port=server_port))
        time.sleep(10)
        # Health-probe the server before opening the UI
        try:
            import requests
            for i in range(30):
                try:
                    r = requests.get(f"http://127.0.0.1:{server_port}/health", timeout=2)
                    if r.status_code == 200:
                        break
                except Exception:
                    pass
                time.sleep(0.5)
        except Exception as e:
            warn(f"[SERVER] health probe exception: {e}")
        # Launch UI
        try:
            import gradio as gr
            import requests
            def chat_fn(message, history):
                payload = {"model": os.path.basename(model_path_qwen), "messages": [{"role": "user", "content": message}], "stream": False}
                try:
                    resp = requests.post(f"http://127.0.0.1:{server_port}/v1/chat/completions", json=payload)
                    resp.raise_for_status()
                    return resp.json()["choices"][0]["message"]["content"]
                except Exception as e:
                    return f"Error: {e}"
            gr.ChatInterface(chat_fn, title="Local LLM Chat (Qwen2.5-Coder-7B)").launch(share=False, server_name="127.0.0.1", server_port=7860)
            log(t("CHAT_UI_LAUNCHED"))
        except Exception as e:
            warn(f"[CHAT_UI] skipped: {e}")
        # Preset script Qwen
        server_bat_qwen = ROOT / ("llama_server_qwen.bat" if OS_TYPE == "windows" else "llama_server_qwen.sh")
        if not server_bat_qwen.exists():
            ubat_line = f"{FLAG_UBAT} %UBS% ^" if FLAG_UBAT else ""
            ngl_line = f"{FLAG_NGL} %NGL% ^" if FLAG_NGL else ""
            tb_line = f"{FLAG_TB} 16" if FLAG_TB else ""
            if OS_TYPE == "windows":
                content = f"""@echo off
setlocal
REM PROFILE: LOW (low VRAM) or HIGH (faster)
if "%Z_PROFILE%"=="" set Z_PROFILE=LOW
set ROOT={ROOT}
set LLAMA_BIN={llama_server}
set MODEL={model_path_qwen}
if /I "%Z_PROFILE%"=="LOW" (
  set NGL=12
  set BS=512
  set UBS=128
) else (
  set NGL=34
  set BS=1024
  set UBS=512
)
set PORT=8080
set THREADS=16
set LLAMA_CUBLAS=1
set GGML_CUDA_FORCE_CUBLAS=1
set GGML_CUDA_ALLOW_TF32=1
set AFF_MASK=0x0FFF
echo PROFILE=%Z_PROFILE%  PORT=%PORT%  NGL=%NGL%  BS=%BS%  UBS=%UBS%
echo BIN: %LLAMA_BIN%
echo MODEL: %MODEL%
start "LLM_SERVER" /affinity %AFF_MASK% /high ^
  "%LLAMA_BIN%" --server --port %PORT% --parallel 4 --cont-batching ^
  --model "%MODEL%" --ctx-size 4096 --batch-size %BS% ^
  {ubat_line}
  {ngl_line}
  --n-predict 512 --temp 0.2 --seed 42 ^
  --threads %THREADS% {tb_line}
"""
            else:
                ubat_shell = f"{FLAG_UBAT} $UBS \\" if FLAG_UBAT else ""
                ngl_shell = f"{FLAG_NGL} $NGL \\" if FLAG_NGL else ""
                tb_shell = f"{FLAG_TB} 16" if FLAG_TB else ""
                content = f"""#!/bin/bash
PROFILE="${{Z_PROFILE:-LOW}}"
ROOT="{ROOT}"
LLAMA_BIN="{llama_server}"
MODEL="{model_path_qwen}"
if [ "$PROFILE" = "LOW" ]; then
  NGL=12
  BS=512
  UBS=128
else
  NGL=34
  BS=1024
  UBS=512
fi
PORT=8080
THREADS=16
export LLAMA_CUBLAS=1
export GGML_CUDA_FORCE_CUBLAS=1
export GGML_CUDA_ALLOW_TF32=1
echo "PROFILE=$PROFILE  PORT=$PORT  NGL=$NGL  BS=$BS  UBS=$UBS"
echo "BIN: $LLAMA_BIN"
echo "MODEL: $MODEL"
"$LLAMA_BIN" --server --port $PORT --parallel 4 --cont-batching \
  --model "$MODEL" --ctx-size 4096 --batch-size $BS \
  {ubat_shell}
  {ngl_shell}
  --n-predict 512 --temp 0.2 --seed 42 \
  --threads $THREADS {tb_shell} &
"""
            server_bat_qwen.write_text(content)
            if OS_TYPE != "windows":
                server_bat_qwen.chmod(0o755)  # Executable
            log(t("SERVER_PRESET_WRITTEN", path=server_bat_qwen))

        # Preset script Mistral
        server_bat_mistral = ROOT / ("llama_server_mistral.bat" if OS_TYPE == "windows" else "llama_server_mistral.sh")
        if not server_bat_mistral.exists():
            ubat_line = f"{FLAG_UBAT} %UBS% ^" if FLAG_UBAT else ""
            ngl_line = f"{FLAG_NGL} %NGL% ^" if FLAG_NGL else ""
            tb_line = f"{FLAG_TB} 16" if FLAG_TB else ""
            if OS_TYPE == "windows":
                content = f"""@echo off
setlocal
REM PROFILE: LOW (low VRAM) or HIGH (faster)
if "%Z_PROFILE%"=="" set Z_PROFILE=LOW
set ROOT={ROOT}
set LLAMA_BIN={llama_server}
set MODEL={model_path_mistral}
if /I "%Z_PROFILE%"=="LOW" (
  set NGL=12
  set BS=512
  set UBS=128
) else (
  set NGL=34
  set BS=1024
  set UBS=512
)
set PORT=8080
set THREADS=16
set LLAMA_CUBLAS=1
set GGML_CUDA_FORCE_CUBLAS=1
set GGML_CUDA_ALLOW_TF32=1
set AFF_MASK=0x0FFF
echo PROFILE=%Z_PROFILE%  PORT=%PORT%  NGL=%NGL%  BS=%BS%  UBS=%UBS%
echo BIN: %LLAMA_BIN%
echo MODEL: %MODEL%
start "LLM_SERVER" /affinity %AFF_MASK% /high ^
  "%LLAMA_BIN%" --server --port %PORT% --parallel 4 --cont-batching ^
  --model "%MODEL%" --ctx-size 4096 --batch-size %BS% ^
  {ubat_line}
  {ngl_line}
  --n-predict 512 --temp 0.2 --seed 42 ^
  --threads %THREADS% {tb_line}
"""
            else:
                ubat_shell = f"{FLAG_UBAT} $UBS \\" if FLAG_UBAT else ""
                ngl_shell = f"{FLAG_NGL} $NGL \\" if FLAG_NGL else ""
                tb_shell = f"{FLAG_TB} 16" if FLAG_TB else ""
                content = f"""#!/bin/bash
PROFILE="${{Z_PROFILE:-LOW}}"
ROOT="{ROOT}"
LLAMA_BIN="{llama_server}"
MODEL="{model_path_mistral}"
if [ "$PROFILE" = "LOW" ]; then
  NGL=12
  BS=512
  UBS=128
else
  NGL=34
  BS=1024
  UBS=512
fi
PORT=8080
THREADS=16
export LLAMA_CUBLAS=1
export GGML_CUDA_FORCE_CUBLAS=1
export GGML_CUDA_ALLOW_TF32=1
echo "PROFILE=$PROFILE  PORT=$PORT  NGL=$NGL  BS=$BS  UBS=$UBS"
echo "BIN: $LLAMA_BIN"
echo "MODEL: $MODEL"
"$LLAMA_BIN" --server --port $PORT --parallel 4 --cont-batching \
  --model "$MODEL" --ctx-size 4096 --batch-size $BS \
  {ubat_shell}
  {ngl_shell}
  --n-predict 512 --temp 0.2 --seed 42 \
  --threads $THREADS {tb_shell} &
"""
            server_bat_mistral.write_text(content)
            if OS_TYPE != "windows":
                server_bat_mistral.chmod(0o755)  # Executable
            log(t("SERVER_PRESET_WRITTEN", path=server_bat_mistral))

        # Preset script OSS20B
        server_bat_oss20b = ROOT / ("llama_server_oss20b.bat" if OS_TYPE == "windows" else "llama_server_oss20b.sh")
        if not server_bat_oss20b.exists():
            ubat_line = f"{FLAG_UBAT} %UBS% ^" if FLAG_UBAT else ""
            ngl_line = f"{FLAG_NGL} %NGL% ^" if FLAG_NGL else ""
            tb_line = f"{FLAG_TB} 16" if FLAG_TB else ""
            if OS_TYPE == "windows":
                content = f"""@echo off
setlocal
REM PROFILE: LOW (low VRAM) or HIGH (faster)
if "%Z_PROFILE%"=="" set Z_PROFILE=LOW
set ROOT={ROOT}
set LLAMA_BIN={llama_server}
set MODEL={model_path_oss20b}
if /I "%Z_PROFILE%"=="LOW" (
  set NGL=12
  set BS=512
  set UBS=128
) else (
  set NGL=34
  set BS=1024
  set UBS=512
)
set PORT=8080
set THREADS=16
set LLAMA_CUBLAS=1
set GGML_CUDA_FORCE_CUBLAS=1
set GGML_CUDA_ALLOW_TF32=1
set AFF_MASK=0x0FFF
echo PROFILE=%Z_PROFILE%  PORT=%PORT%  NGL=%NGL%  BS=%BS%  UBS=%UBS%
echo BIN: %LLAMA_BIN%
echo MODEL: %MODEL%
start "LLM_SERVER" /affinity %AFF_MASK% /high ^
  "%LLAMA_BIN%" --server --port %PORT% --parallel 4 --cont-batching ^
  --model "%MODEL%" --ctx-size 4096 --batch-size %BS% ^
  {ubat_line}
  {ngl_line}
  --n-predict 512 --temp 0.2 --seed 42 ^
  --threads %THREADS% {tb_line}
"""
            else:
                ubat_shell = f"{FLAG_UBAT} $UBS \\" if FLAG_UBAT else ""
                ngl_shell = f"{FLAG_NGL} $NGL \\" if FLAG_NGL else ""
                tb_shell = f"{FLAG_TB} 16" if FLAG_TB else ""
                content = f"""#!/bin/bash
PROFILE="${{Z_PROFILE:-LOW}}"
ROOT="{ROOT}"
LLAMA_BIN="{llama_server}"
MODEL="{model_path_oss20b}"
if [ "$PROFILE" = "LOW" ]; then
  NGL=12
  BS=512
  UBS=128
else
  NGL=34
  BS=1024
  UBS=512
fi
PORT=8080
THREADS=16
export LLAMA_CUBLAS=1
export GGML_CUDA_FORCE_CUBLAS=1
export GGML_CUDA_ALLOW_TF32=1
echo "PROFILE=$PROFILE  PORT=$PORT  NGL=$NGL  BS=$BS  UBS=$UBS"
echo "BIN: $LLAMA_BIN"
echo "MODEL: $MODEL"
"$LLAMA_BIN" --server --port $PORT --parallel 4 --cont-batching \
  --model "$MODEL" --ctx-size 4096 --batch-size $BS \
  {ubat_shell}
  {ngl_shell}
  --n-predict 512 --temp 0.2 --seed 42 \
  --threads $THREADS {tb_shell} &
"""
            server_bat_oss20b.write_text(content)
            if OS_TYPE != "windows":
                server_bat_oss20b.chmod(0o755)  # Executable
            log(t("SERVER_PRESET_WRITTEN", path=server_bat_oss20b))
    # Audit
    log(t("AUDIT_VERIFY"))
    if audit_install(ROOT, {"cli": llama_cli, "server": llama_server}, model_path_qwen, server_port):
        log(t("AUDIT_COMPLETE"))
    else:
        err("[AUDIT] Issues found; check logs.")

# SIGTERM/SIGINT cleanup
server_proc = None
atexit.register(_kill_child, lambda: server_proc)
signal.signal(signal.SIGTERM, lambda *_: (_kill_child(server_proc), sys.exit(0)))
signal.signal(signal.SIGINT,  lambda *_: (_kill_child(server_proc), sys.exit(0)))

# CLI app
app = typer.Typer(add_completion=False)

@app.command()
def install(no_ui: bool = typer.Option(False, "--no-ui", help="Skip launching Gradio UI"),
            port: int = typer.Option(8080, "--port", help="API server port")):
    # ... (grow with opts; for now use defaults)
    install_all()

@app.command()
def service(action: str):
    if action == "install":
        service_install()
    elif action == "start":
        service_start()
    elif action == "stop":
        service_stop()

if __name__ == "__main__":
    if len(sys.argv) == 1:
        log(t("DEFAULT_TO_INSTALL"))
        sys.argv.append("install")  # Append 'install' to args to default to install command
    app()
